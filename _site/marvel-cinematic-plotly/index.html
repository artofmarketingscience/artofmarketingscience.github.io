<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">
    
<!-- <title>Marvel Cinematic Data Visualization With Plot.ly | The Art of Marketing Science</title>-->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
    
<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
    
<link href="/assets/css/screen.css" rel="stylesheet">
    
<link href="/assets/css/main.css" rel="stylesheet">
    
<script src="/assets/js/jquery.min.js"></script>

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Marvel Cinematic Data Visualization With Plot.ly | The Art of Marketing Science</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Marvel Cinematic Data Visualization With Plot.ly" />
<meta name="author" content="data_sandbox" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="3D network graph of Marvel characters using Plot.ly" />
<meta property="og:description" content="3D network graph of Marvel characters using Plot.ly" />
<meta property="og:site_name" content="The Art of Marketing Science" />
<meta property="og:image" content="/assets/images/2017-06-01-marvel-cinematic-plotly/marvel-cinematic.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-06-01T00:00:00-04:00" />
<script type="application/ld+json">
{"description":"3D network graph of Marvel characters using Plot.ly","author":{"@type":"Person","name":"data_sandbox"},"@type":"BlogPosting","url":"/marvel-cinematic-plotly/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"},"name":"data_sandbox"},"image":"/assets/images/2017-06-01-marvel-cinematic-plotly/marvel-cinematic.png","headline":"Marvel Cinematic Data Visualization With Plot.ly","dateModified":"2017-06-01T00:00:00-04:00","datePublished":"2017-06-01T00:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"/marvel-cinematic-plotly/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
</head>
    

    

<body class="layout-post">

    
<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">
    
    <div class="container pr-0">    
    
    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="The Art of Marketing Science">
    </a>
    <!-- End Logo -->
  
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    
    <div class="collapse navbar-collapse" id="navbarMediumish">
       
        <!-- Begin Menu -->
        
            <ul class="navbar-nav ml-auto">
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Home</a>
                </li>
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/about">About</a>
                </li>
                
                <li class="nav-item">
                <a class="nav-link" href="/contribute">Contribute</a>
                </li>
            
                
                <script src="/assets/js/lunr.js"></script>

<script>
    

var documents = [{
    "id": 0,
    "url": "/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "/about",
    "title": "About The Art of Marketing Science",
    "body": "	The Art of Marketing Science is a blog focused on applying data science in marketing, sharing from concepts to practical code for readers. Here we explore different ways to apply data science to marketing problems, think about marketing data, highlight challenges that industry marketing data scientists have faced, and implement various algorithms in Python &amp; R. The purpose is to bridge the gaps between theory and application, concepts and implementation and data scientists and marketers.  	Data science is applied widely in marketing as every company on this planet is looking to better understand their marketing efforts in hopes to reduce inefficiencies and maximize effectiveness but there is a lack of consolidated resources covering practical application of data science in marketing. This blog is geared towards data scientist currently working in marketing and marketing analysts looking to expand beyond traditional analytics. The Author	My name is Jason Fong (more popularly referred to as fongmanfong) and I am a Data Scientist focused on solving marketing problems. Currently I am heading up Growth Algorithms at Wealthsimple, a Toronto based Fintech start up. Previously I lead the Marketing Data Science team at Shopify, an e-commerence platform helping scale the platform from 200K merchants to over 1M merchants.  	My job is pretty straightforward: help marketing make more data informed decisions to reduce cost of acquisition and increase revenue for Shopify. How do I do this with my team? Well that is not so straightforward. We're working on things like:	Building and productionizing an algorithmic (moving beyond just heuristic based) attribution model	Media Mix Modelling to complement multi-touch attribution model	Geo Experimentation to understand causal impact of marketing campaigns	Building a state-of-the-art data infrastructure with scalable data pipelines to produce high quality datasets 	Statistical Modelling &amp; Machine Learning (churn, LTV, propensity, lead scoring etc)	A/B testing and experiments	Causal inference and regression modelling to understand what levers move the needle 	I'm also one of the co-founders at MUDAVEN, where we take a technology first approach to real estate. We help real estate agents with creating 3D virtual tours, modern web design, digital marketing and more. Let's ConnectIf you've read up to this point, I'd love to connect and see how we can collaborate. You DM me on any of the social platforms "
    }, {
    "id": 2,
    "url": "/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "/contribute",
    "title": "Contribute To The Art of Marketing Science",
    "body": "		Marketing is a complex domain and I am just scratching the surface here. My hope is that readers will learn something from this blog and I can connect with other folks working in this domain space.  	Are you currently a data scientist in marketing? Do you have specific marketing science knowledge that you'd love to share? This blog is currently open to guest blogging! If you're interested in contributing as a guest blogger 	1) Please open up an issue in the github repo describing your topic. 	2) Submit your article and/or Jupyter/R notebook	3) I'll Peer review prior to merging	Similarily, if there is a topic you'd like to discuss or want me to write about, please open up an issue in the github repo. "
    }, {
    "id": 4,
    "url": "/",
    "title": "Data Science in Marketing",
    "body": "  {{ site. name }}       {{ site. description }}   {% if page. url == “/” %}       Featured:           {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}      {% endif %}       All Stories:         {% for post in paginator. posts %}        {% include postbox. html %}    {% endfor %}          {% include pagination. html %}"
    }, {
    "id": 5,
    "url": "/DS-dimension_reduction_examples_part1/",
    "title": "",
    "body": ""
    }, {
    "id": 6,
    "url": "/DS-dimension_reduction_pca/",
    "title": "",
    "body": ""
    }, {
    "id": 7,
    "url": "/DS-marvel_cinematic_plotly/",
    "title": "",
    "body": ""
    }, {
    "id": 8,
    "url": "/DS-regression_modelling_strategies/",
    "title": "",
    "body": ""
    }, {
    "id": 9,
    "url": "/AOM-from_marketing_analytics_to_marketing_science/",
    "title": "",
    "body": ""
    }, {
    "id": 10,
    "url": "/AOM-one_thing_marketing_analysts_should_have_in_their_analytics_toolkit/",
    "title": "",
    "body": ""
    }, {
    "id": 11,
    "url": "/AOM-geo-experiments-part1/",
    "title": "",
    "body": ""
    }, {
    "id": 12,
    "url": "/problems_before_tools_an-alternative_view_to_generalist_data_scientist/",
    "title": "",
    "body": ""
    }, {
    "id": 13,
    "url": "/www.artofmarketingscience.github.io /DS-kaggle_titanic/",
    "title": "",
    "body": ""
    }, {
    "id": 14,
    "url": "/redirects.json",
    "title": "",
    "body": "{“/DS-dimension_reduction_examples_part1/”:”/dimension-reduction-correlation-low-variance-filter/”,”/DS-dimension_reduction_pca/”:”/dimension-reduction-pca/”,”/DS-marvel_cinematic_plotly/”:”/marvel-cinematic-plotly/”,”/DS-regression_modelling_strategies/”:”/vanderbilt-university-regression-modelling-strategies-course/”,”/AOM-from_marketing_analytics_to_marketing_science/”:”/from-marketing-analytics-to-marketing-science/”,”/AOM-one_thing_marketing_analysts_should_have_in_their_analytics_toolkit/”:”/regression-must-have-for-marketing-analysts/”,”/AOM-geo-experiments-part1/”:”/what-are-geo-experiments/”,”/problems_before_tools_an-alternative_view_to_generalist_data_scientist/”:”/problems-before-tools-story-full-stack-data-scientist/”,”/www. artofmarketingscience. github. io /DS-kaggle_titanic/”:”/”} "
    }, {
    "id": 15,
    "url": "/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 16,
    "url": "/detecting-conversion-changes-using-bayesian-change-point/",
    "title": "Detecting Conversion Change Using Bayesian Change Point",
    "body": "2020/12/08 - In this blog article we’ll explore a fairly simple yet powerful analysis method called Bayesian Change Point analysis. We’ll apply this nifty tool to a real life analysis (using simulated data) I did at Shopify to detect changes in marketing conversions. The model is built in PyMC3 and the Python code that is included demonstrates both a single change point as well as a double change point. What is Bayesian Change Point Analysis?: Bayesian Change Point or Bayesian Switchpoint analysis is a method used to detect whether the mean, variance or periodicity of data changed abruptly at some point in time and when that change occured. I won’t be going into detail on the math behind the analysis and how to set the priors for this Bayesian analysis. I’ll likely leave that for another article. A lot of the code shared here is borrowed from my previous Director &amp; mentor Cameron Davidson Pilon’s book Bayesian for Hackers. I’d highly encourage taking a read for more information. I’ll be adding additional example to illustrate how to include multiple switchpoints in the Bayesian Change Point model. The Shopify COVID-19 Story: In Growth Marketing, we pay close attention to the marketing funnel and the conversions at each step of the funnel. At Shopify, signing up for a free trial is one of the main marketing conversions the team focuses on monitoring and optimizing. This is a very important conversion point as it is highly correlated to how many merchants are signing up onto the Shopify platform and Shopify’s overall growth. In March of 2020, when the world started to shut down in response to the COVID-19 pandemic, we saw abnormal changes in various KPIs; free trial sign-up being one of metrics that were impacted. There were worries across the executive leadership team that sign-up will be negatively impacted with the world shutting down. At the same time, Shopify decided to extend its 14-day free trial to 90 days to help small businesses get through the hard times. The worries turned out to be wrong and, surprisingly, the opposite happened. It actually appeared as if sign-ups increased across different countries at similar but slightly different times during March. In hindsight, the need to move online is fairly clear and an increase in sign-ups makes sense, but it was definitely not clear at that moment in time. I approached this problem using Bayesian Change Point model to understand if there was actually a change in sign-ups and if so, inferred the probability of when it happened. What I found from the analysis was highly interesting. Firstly, we are very confident that there was an abrupt change in sign-ups during March. Secondly and more interestingly, the change in sign-ups across different countries aligned exactly on the dates of when those countries went into nationwide lockdown. For a bit more context, Solmaz Shahalizadeh, VP of Data Science and Engineering, briefly talks about this insight on the Life @ Shopify podcast. I extended this Bayesian Switchpoint analysis to understand if the introduction of the 90-day free trial had an effect on the change in sign-ups. I modelled two switches into the analysis and low and behold the model was able to detect two changes, one from nationwide lockdown and the other from 90-day free trial, both switches occuring on explanable dates! %matplotlib inlineimport pandas as pdimport pymc3 as pmimport randomimport numpy as npfrom IPython. core. pylabtools import figsizefrom matplotlib import pyplot as pltimport warningswarnings. filterwarnings('ignore')Detecting one switchpoint: Let’s begin by simulating some sign-up data to run our Bayesian Change Point model on. We mock up sign-up data across 31 days in March with one change in average daily sign ups some time in March due to lockdown. I’ve fixed the change to occur some time in mid-March. The change could have happened in early March or tail end of March and we can easily extend the time horizon into April in our real analysis. mu_1 = 1000 # average daily sign up pre lockdownmu_2 = 1100 # average daily sign up post lockdownnum_days = 31lockdown = random. randint(12, 20) # simulating the change sometime mid March. print (lockdown)20# Simulate March sign up datasignup_pre_lock = np. random. poisson(lam=mu_1, size=(lockdown))signup_post_lock = np. random. poisson(lam=mu_2, size = (num_days - lockdown))signup = np. concatenate([signup_pre_lock, signup_post_lock], axis=0)num_days == len(signup) # Check to make sure days line upTruefigsize(12. 5, 3. 5)plt. bar(np. arange(len(signup)), signup, color= #348ABD )plt. xlabel( Day )plt. ylabel( Sign Ups )plt. title( Did Shopify Sign Up Change During March? )plt. xlim(0, len(signup)); For more details around syntax of PyMC3, I’d encourage you to read through documentation or Bayesian for Hackers. In our PyMC3 model, we define the priors of each of the parameters we are inferring, the proposed data generation scheme using switch function, and tying our observed sign up data with data generation scheme. with pm. Model() as model:  alpha = 1. 0/signup. mean() # See Bayesian for Hackers  signup_pre_lock_param = pm. Exponential( signup_pre_lock_param , alpha) # Prior - Average daily sign up pre lockdown  signup_post_lock_param = pm. Exponential( signup_post_lock_param , alpha) # Prior - Average daily sign up post lockdown    lockdown_param = pm. DiscreteUniform( lockdown_param , lower=0, upper=num_days - 1) #Prior - Day of when switch occured     idx = np. arange(num_days) # Index  signup_param = pm. math. switch(lockdown_param &gt; idx, signup_pre_lock_param, signup_post_lock_param) # Model a switch after a certain point in time    observation = pm. Poisson( obs , signup_param, observed=signup) # Feed model observationswith model:  trace = pm. sample(10000, tune=5000, chains=4)Multiprocess sampling (4 chains in 2 jobs)CompoundStep&gt;NUTS: [signup_post_lock_param, signup_pre_lock_param]&gt;Metropolis: [lockdown_param]    100. 00% [60000/60000 01:22&lt;00:00 Sampling 4 chains, 0 divergences]Sampling 4 chains for 5_000 tune and 10_000 draw iterations (20_000 + 40_000 draws total) took 94 seconds. The number of effective samples is smaller than 25% for some parameters. pm. traceplot(trace); The traceplot provides a summary of the sampling. On the left is the posterior distribution of the parameters we are inferring and on the right highlights how well the sampling “mixed”. What you’re looking for is convergence of the distribution as well as the traces mixing well. A more visual way to put it, you want your mix plot look like a caterpillar. As we see belong, the Bayesian Swtichpoint model is able to correctly infer when the switch occured, the average daily sign ups before and after the switch. signup_pre_lock_param_samples = trace['signup_pre_lock_param']signup_post_lock_param_samples = trace['signup_post_lock_param']lockdown_param_samples = trace['lockdown_param']prob_lock_post_greater_pre = (signup_post_lock_param_samples &gt; signup_pre_lock_param_samples). mean()prob_lockdown_day = (lockdown_param_samples == lockdown). mean()print ( Average Sign Up Before Switch : %s. vs. actual %s.   % (round(signup_pre_lock_param_samples. mean(), 0), mu_1))print ( Average Sign up After Swtich : %s. vs. actual %s.   % (round(signup_post_lock_param_samples. mean(), 0), mu_2))print ( Day of Switch : %s. vs. actual %s.   % (round(lockdown_param_samples. mean(), 0), lockdown - 1))print ( Probability Signup Post &gt; Pre : %s.   % (round(prob_lock_post_greater_pre, 2)))print ( Probability Switch on Day %s. : %s.   % (lockdown, prob_lockdown_day))Average Sign Up Before Switch : 982. 0. vs. actual 1000. Average Sign up After Swtich : 1112. 0. vs. actual 1100. Day of Switch : 20. 0. vs. actual 18. Probability Signup Post &gt; Pre : 1. 0. Probability Switch on Day 19. : 0. 00095. figsize(12. 5, 3)N = lockdown_param_samples. shape[0]signups_per_day = np. zeros(num_days)for day in range(0, num_days):  ix = day &lt; lockdown_param_samples  signups_per_day[day] = (signup_pre_lock_param_samples[ix]. sum() + signup_post_lock_param_samples[~ix]. sum()) / Nplt. plot(range(num_days), signups_per_day, lw=4, color= #E24A33 ,     label= Expected Shopify Sign Ups Per Day )plt. xlim(0, num_days)plt. xlabel( Day )plt. ylabel( Shopify Sign Ups )plt. title( Expected Shopify Sign Ups Per Day )plt. bar(np. arange(num_days), signup, color= #348ABD , alpha=0. 65,    label= Oberseved Sign Ups )plt. vlines(lockdown, 0, 1200, colors='black', linestyles='solid', label='Lockdown')plt. legend(loc= lower left ); Detecting multiple switchpoints: We can easily extend our Bayesian Change Point detection to model two switchpoints. We’ll simulate an additional switchpoint in sign-ups due to the 90-day free trial. Instead of modelling pre- and post-, we will have an additional parameter for sign-ups post free trial introduction as well as an additional day of when switch occurs. mu_3 = 1200 # average daily signups after 90d free trialnum_days = 45 # extend out to look at 1. 5 months of signupslockdown = random. randint(12, 20) # simulating the change sometime mid March. free_trial = random. randint(25, 35) # simulating the change late March - early Aprilprint (lockdown)print (free_trial)1933# Simulate March sign up datasignup_pre_lock = np. random. poisson(lam=mu_1, size=(lockdown))signup_post_lock = np. random. poisson(lam=mu_2, size = (free_trial - lockdown))signup_post_trial = np. random. poisson(lam=mu_3, size = (num_days - free_trial))signup = np. concatenate([signup_pre_lock, signup_post_lock, signup_post_trial], axis=0)num_days == len(signup) # Check to make sure days line upTruewith pm. Model() as model_2:    alpha = 1. 0/signup. mean() # See Bayesian for Hackers    signup_pre_lock_param = pm. Exponential( signup_pre_lock_param , alpha) # Prior - Average daily sign up pre lockdown  signup_post_lock_param = pm. Exponential( signup_post_lock_param , alpha) # Prior - Average daily sign up post lockdown  signup_post_trial_param = pm. Exponential( signup_post_trial_param , alpha) # Prior - Average daily sign up post free trial    lockdown_param = pm. DiscreteUniform( lockdown_param , lower=0, upper=num_days - 1) # Prior - Day of when lockdown switch occured   trial_param = pm. DiscreteUniform( trial_param , lower=lockdown_param, upper=num_days - 1) # Prior - Day of when free trial switch occured    idx = np. arange(num_days) # Index    signup_param = pm. math. switch(trial_param &gt;= idx, pm. math. switch(lockdown_param &gt;= idx, signup_pre_lock_param, signup_post_lock_param), signup_post_trial_param) # Model a switch after a certain point in time    observation = pm. Poisson( obs , signup_param, observed=signup) # Feed model observationswith model_2:  trace_2 = pm. sample(10000, tune=5000, chains=4)Multiprocess sampling (4 chains in 2 jobs)CompoundStep&gt;NUTS: [signup_post_trial_param, signup_post_lock_param, signup_pre_lock_param]&gt;CompoundStep&gt;&gt;Metropolis: [trial_param]&gt;&gt;Metropolis: [lockdown_param]    100. 00% [60000/60000 01:09&lt;00:00 Sampling 4 chains, 0 divergences]Sampling 4 chains for 5_000 tune and 10_000 draw iterations (20_000 + 40_000 draws total) took 79 seconds. The number of effective samples is smaller than 25% for some parameters. pm. traceplot(trace_2); # Draw double switch graphsignup_pre_lock_param_samples = trace_2['signup_pre_lock_param']signup_post_lock_param_samples = trace_2['signup_post_lock_param']signup_post_trial_param_samples = trace_2['signup_post_trial_param']lockdown_param_samples = trace_2['lockdown_param']trial_param_samples = trace_2['trial_param']figsize(12. 5, 3)N = lockdown_param_samples. shape[0]signups_per_day = np. zeros(num_days)for day in range(0, num_days):  ix = day &lt;= lockdown_param_samples  ix2 = (lockdown_param_samples &lt; day) &amp; (day &lt;= trial_param_samples)  signups_per_day[day] = (signup_pre_lock_param_samples[ix]. sum() +               signup_post_lock_param_samples[ix2]. sum() +              signup_post_trial_param_samples[(~ix2 &amp; ~ix)]. sum()              ) / Nplt. plot(range(num_days), signups_per_day, lw=4, color= #E24A33 ,     label= Expected Shopify Sign Ups Per Day )plt. xlim(0, num_days)plt. xlabel( Day )plt. ylabel( Shopify Sign Ups )plt. title( Expected Shopify Sign Ups Per Day )plt. bar(np. arange(num_days), signup, color= #348ABD , alpha=0. 65,    label= Oberseved Sign Ups )plt. vlines(lockdown, 0, 1100, colors='black', linestyles='solid', label='Lockdown')plt. vlines(free_trial, 0, 1300, colors='yellow', linestyles='solid', label='Free Trial')plt. legend(loc= lower left ); Things to Notes:: The Bayesian Switchpoint analysis is a simple and useful tool to have in your repertoire. Inferring the probability of a switchpoint and when it occurs can be particularly useful when you have a predefined hypothesis prior to running the model. For example, I had a prior belief that there may be an impact on sign-up because of countries going into lockdown and the introduction of the 90-day free trial. When the inference matches my hypothesis across various different countries, it gives me stronger confidence that my hypothesis is correct. It’s also important to note that the Bayesian Switchpoint model doesn’t answer any questions around causality or incrementality. Meaning I can’t really use this tool to answer how many incremental customers the 90-day free trial generated. Lastly, as with any analysis dependent on time, the time range selected for analysis is important. This highly depends on your domain knowledge and context on the scope of the problem. For example, it would be naive to assume the lockdown has a multi-year effect on sign-up. This is why we limited to the scope of analysis close to the time of the event. "
    }, {
    "id": 17,
    "url": "/problems-before-tools-story-full-stack-data-scientist/",
    "title": "Problems before tools: A story of the full-stack data scientist",
    "body": "2019/03/29 -  I recently read an article from Eric Colson, Chief Algorithm Officer at Stitch Fix, where he talked about how we should avoid building data science teams like a manufacturing plant, comprising of highly specialized individuals operating different parts of a manufacturing process. Instead data science teams should be built with a full stack approach where data scientists are considered to be generalists. Generalists refers to the capability of performing diverse functions from conception to modelling to implementation to measurement. I won’t go into a detail summary of the article here but you should read Eric’s article before continuing on. The purpose of this article is to provide a complementary view into Eric’s philosophy. In his article, he took a very top down approach to describing why a data science team should be built with generalists. I believe the same conclusion can be drawn through the lens of a bottoms up approach, from a perspective of a practitioner of data science and what it really means to be in data science. Let’s start this discussion of with just defining data science. What exactly is data science or what does a data scientist do? Looking to our friendly neighbourhood Mr. Interweb for some help, here are some definitions that I’ve found. Tell Me Sir, What Is This Data Science You Refer To?: “Data science is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. ” – Wikipedia “Data science is a “concept to unify statistics, data analysis, machine learning and their related methods” in order to “understand and analyze actual phenomena” with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science. ” – Wikipedia Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract value from data. Data scientists combine a range of skills—including statistics, computer science, and business knowledge—to analyze data collected from the web, smartphones, customers, sensors, and other sources. - Oracle “Data science is the field of study that combines domain expertise, programming skills, and knowledge of math and statistics to extract meaningful insights from data. Data science practitioners apply machine learning algorithms to numbers, text, images, video, audio, and more to produce artificial intelligence (AI) systems that perform tasks which ordinarily require human intelligence. In turn, these systems generate insights that analysts and business users translate into tangible business value. ” – Data Robot These are some nicely crafted definitions (way better than I would have been able to articulate it) but an observable and consistent pattern across these definitions (along with numerous other on the internet) is that data science tends to be defined by doing a set of specific tasks or using a set of specific tools. If you do X, Y, Z and use A, B, C then that’s considered data science. The causal chain would look something this: Data Science - The Most Unsexy Field In The 21st Century Practicing data science will lead you to use math and statistics, apply machine learning algorithm, learn programming, increase domain expertise. Why? First, I think through the process of packaging up and marketing data science to be this sexy field, we have incorrectly defined the definition of data science. I don’t think the definitions mentioned above are incorrect but I think a definition like this one is more authentic: “Data science, in its most basic terms, can be defined as obtaining insights and information, really anything of value, out of data. Like any new field, it’s often tempting but counterproductive to try to put concrete bounds on its definition. ” – Thinkful To make the definition even less sexy, I believe data science should be simply: “Just using data to solve problems”. Data scientist should be “the data girl or the data guy”. Not so sexy anymore. Though we still have all the same elements in the two causal chains, the subtle difference in reversal of causality actually leads to a big difference in how data science is practiced in reality. This is the same concept as viewing a cup of water as half full vs. half empty. Although both views are correct, there is a significant difference in how individuals that see things as half full behave and act versus individuals that sees it as half empty. In the world through the first causal chain, we see a lot of data science practitioners that are solely interested in specific tasks of data science. Have you met anybody that explicitly indicated they are only interested in building models and don’t care about analytics or working with stakeholders? Odds are you probably have. In the world through the second causal chain, specific data science tasks are performed because it helps solve the problem at hand. How Does This Relate to Being A Generalist?: So, after all of this detour, how does any of this stuff relate to being a generalist? I believe if we looked at data science the right way, the only way to really do data science optimally is to be a generalist (once again, love to hear your thoughts and opinions in the comments below). Why? If we always start with the problem being the main focus, we will realize that if we wanted to solve the problem the best way possible, we cannot simply just do one part of data science and not the other part. Echoing Eric in his article, a generalist role provide all the things that drive job satisfaction: autonomy, mastery and purpose. Especially around mastery, if we always start with the problem, we can determine if our current toolkit can adequately answer the question or we need to branch deeper and apply new data science techniques or technology to help address the problem. "
    }, {
    "id": 18,
    "url": "/what-are-geo-experiments/",
    "title": "What are Geo Experiments and how it is used in marketing",
    "body": "2019/03/25 - This will be a three-part series discussing the topic of Geo Experiments and it’s use in marketing. Part 1: What Is It and How Will It Help You In Marketing? Part 2: Understanding the mathematics behind Geo Experiments Part 3: Application of Geo Experiments with examples and R code The Bread &amp; Butter of Test &amp; Learn: A/B testing (aka split testing) has been essential in helping marketers remove the guesswork and make data-informed decisions. For most marketers, this comes in the form of landing page optimization. You split traffic into different groups, expose some changes to the page to one of the groups while keeping everything else the same for the other, measure the difference for a specific metric of interest (signup rate, CTR etc) and determine if the difference between the two groups is statistically significant. Some paid marketing specialists may have worked with A/B testing in Google Ads or Facebook Ads, where the audience is subjected to different ad formats or ad copy. Similar methodology for both but applied differently. So why do marketers run A/B tests? This is where marketers put on their scientist hat. Marketers run A/B testing to understand the causal and incremental impact of something, whether it be a change on a landing page or ad copy for their Facebook campaign. They want to know, for this specific thing that they did, what is the incremental impact that it had, so that they can make informed decisions about what they should do next. A/B testing has been the bread and butter for marketers to do tests and learn. But if I were to ask you to design an A/B test to answer each of the following questions, how would you go about doing so?  Understand the incremental impact of a Facebook campaign on customer conversion? (Meaning, I want to know the actual impact on overall customer conversion and not what’s reported in your Facebook Business Manager under the conversion column) Understand the effect of billboard advertisement on visits to your websiteGive it some thought.  Let’s go through each question and see why our favourite A/B test doesn’t work:    You can A/B test your Facebook campaign to understand the incremental impact for metrics like CTR or engagement rate but customer conversion is a little different. Why? Because under your whole measurement framework lies an attribution model that assigns credit in a non-perfect way. As a result, even though you are running an A/B test to understand customer conversion, it only provides you with a relative comparison between the two campaigns. It does not provide an overall view of whether the campaign brought in incremental customers. You do not know if whether you’re simply taking customer conversions away from other channels.     This one is straightforward. Firstly, you just don’t have any direct response metrics to measure. Secondly, how do you plan to create your control and test group? Do you plan to blindfold half the population in a city that passes by the billboard to ensure they don’t see the billboard while the letting other half see it?  A/B testing does not work when you do not have control over your population or have a direct measure of the metric of interest. Although A/B testing is the bread and butter for marketers we also see it’s limitations and the need for additional tooling to help marketers answer important questions. The Bread &amp; Olive Oil: Geo Experiments: Sometimes bread and butter just isn’t enough and you got to switch things up. Bread and olive oil usually goes well together too. So what exactly are Geo Experiments? Well, in the simplest terms, Geo Experiments are like A/B tests but uses geographies to define your control and treatment group rather than individuals or web cookies. In Geo Experiments, geographic regions are divided into treatment and control group. The regions in the treatment group are exposed to a marketing intervention while regions in the control group remain status quo. The marketing intervention happens for a duration of time and the response metric is observed. The idea is to detect if there is an incremental lift in the response metric in the treatment regions. Let’s use the Facebook question above as an example and assume we are running the ad campaign in the United States. First, we randomize a set of cities in the United States putting them into treatment and control group. Just like A/B tests, randomization is a critical part of the experimental design. Then in Facebook Business Manager, we set the campaign to only run in the cities in the treatment group. We run the campaign for a duration of time and we measure the incremental customers acquired (agnostic of an attribution model) by comparing the difference between the control vs. treatment group. For example, we may see something like: This is a graph produced by the GeoexperimentsResearch R package developed at Google which implements the Geo Experiment methodology and provides an easy way to perform the analysis. I’ll be demonstrating how to use this R package in Part 3! But at a high level, the y-axis can be customer conversion for our example purposes, the x-axis is divided into three distinct periods:  Pre-test period (Feb 05 — Mar 31): before the campaign started Test period (Apr 01 — Apr 28): when the campaign is running Cooldown period (Apr 29 — May 05): campaign stops running but there might be a lingering effect of the ad that trickle inThe top graph illustrates the observed response metric overtime vs the predicted counterfactual. We’ll talk more about this in detail in Part 2 but essentially the counterfactual is what we would have observed if the marketing campaign did not run. The middle graph illustrates the difference between the observed and counterfactual by day which estimates the lift by day. Finally, the bottom graph is the total lift we’ve observed in the experiment. My explanation above is an oversimplification of the methodology. To truly understand what’s going on, you need to go inside the hood and look at the engine. The methodology and math behind Geo Experiment are different from a traditional A/B test but the overall idea is similar. As mentioned, I will be leaving the detailed explanation of the mathematics to Part 2 of this series. The methodology that I’ll be going through in Part 2 is research from Google, so if you are interested, take a look first. A one-liner explanation: a regression model is used to learn the exchangeability factor between the control regions and the treatment regions and then used to predict the counterfactual during the intervention period. Maybe this sentence doesn’t make any sense to you at all. Don’t worry, I will be breaking this down in simple terms! Stay tuned! I want to take a short moment to highlight that Geo Experiments are another use case of regression in marketing science. If you haven’t read my article “Regression, a must have for Marketing Analysts”, take a look to see why regression is a tool every Marketing Analyst should have in their toolkit. By no means are Geo Experiments designed to replace traditional A/B test. They serve very different purposes and answers different questions but both are equally important. Just as they say, “there is no one size fits all”, you shouldn’t just have butter with your bread. Go ahead and add some variety, olive oil is great too! Challenges and Limitations: By now, I might have sold you on how great this new tool is BUT, like any other tools, Geo Experiment comes with its own set of limitations and challenges:  Overhead Cost: There may be a lot more work involved to set up a Geo Experiments depending on how you’ve set up your campaigns. You may need to restructure your existing digital marketing accounts in a way that allows you to target campaigns at a city/region level to create your control and treatment group.  Platform: Not every marketing platform you advertise on allows you to target campaigns at a city/region level. For example, you may be able to target Facebook paid ads at a city/region level but you don’t have that capability when you are running ads in podcasts.  Budget: Depending on your business, campaign, and many other factors, the budget you need for a campaign will greatly vary. For companies that are just starting out with digital marketing, I wouldn’t be looking at Geo Experiments just yet. If your business is mature and you’re looking to optimize, then I think its a good fit.  Controlling for variables: Can you control for marketing activities happening across different cities? E. g are there other marketing initiatives from your organization happening in specific cities that might overlap with your Geo Experiment?And finally, alway’s remember to choose the right tool for the right job. There is no one size fits all! "
    }, {
    "id": 19,
    "url": "/regression-must-have-for-marketing-analysts/",
    "title": "Regression, a must have for Marketing Analysts",
    "body": "2018/12/31 - Let’s be real here, being a data scientist ain’t easy. Have you seen what a modern data scientist should know?! 😱 That’s a pretty extensive list of requirements but how feasible and practical is it to know everything on this list? In my opinion, not that feasible nor practical. In reality, a data scientist shouldn’t need to know everything mentioned (at least with the same level of competency). Instead, the problem space that a data scientist is working in should define the set of tools he/she needs. Similar to doctors, you either have family doctors who are generalists or you have doctors specializing in different areas. You don’t expect one doctor to know everything and be a specialist in everything. So for a marketing analyst or data scientist in marketing, what are some tools they should have in their toolkit? Most of the time we hear about things like machine learning, R, Python, domain knowledge, communication skills etc. but one tool that has not gotten enough spotlight in marketing is regression. To answer why regression should be in every marketing analysts analytics toolkit, we should first think about what makes a good tool for marketing. What Makes A Good Tool For Marketing?: And to answer this question, we should think about what problems or questions a marketing organization or a CMO needs to answer. Some bigger questions include:  What is the growth and revenue for the company? What is a customer’s lifetime value? Who is at risk of churning? Who should we send this promotional offer to? What is the ROI of my advertising investment?In theory, a good tool should help with answering some of these questions (but it would be naive to think that there is one tool that can solve all the problems aka No Free Lunch Theorem). In addition to just answering these questions, what’s equally as important for a CMO is understanding what levers are moving the needle.  For example, it’s not enough to know whether a company is growing or if revenue is increasing, a CMO needs to know what is moving the needle. Who is at risk of churning and what signals indicate a high risk of churning? Therefore a good tool in marketing should not only help with answering some of these questions but also provide insight into what variables are moving the needle. What is Regression?: I won’t be going into any details on regression in this article. I have some resources at the end if you are interested in learning more about regression. At a high-level, regression is a family of statistical analysis techniques that examine the relationships between variables. There are different types of regressions (logistic, linear, survival etc) in the family but the overall objective is to model the relationship between variables. For example, we may be interested in understanding how investment in each marketing channel influences customer acquisition. We can build a model where the independent variables (factors you hypothesize to have a relationship) is the investment in each marketing channel and the dependent variable (factors you are trying to understand) being customers acquired. It could hypothetically look something like Customers Acquired = B0 + B1 * AdWords_Spend + B2 * Facebook_Spend + B3 * Snapchat_Spend Oh No, Equations and Formulas 😰: The beauty of regression lies in the equation. Why? Because if we’ve modeled the data appropriately, we can make interpretations of the coefficients (the Betas) in the equation to make decisions. This is different than some of the more popular machine learning techniques like deep learning, random forest etc. which are powerful tools for making predictions but the blackbox nature of the method makes it not interpretable. Depending on how the data is modeled (e. g. logistic vs linear vs log-log) we can interpret the coefficient in different ways to understand the relationship between a dependent variable to the independent variable. Let’s take the equation above as an example: we’ve used linear regression to model marketing spend versus customers acquired. The coefficients can be interpreted as a dollar spent in Google Adwords gets 2 number of customers. Customers Acquire = 100+ 2. 0 * AdWords_Spend + 1. 3 * Facebook_Spend + 0. 5 * Snapchat_Spend In other words, the coefficients can be interpreted as the levers that a CMO would be interested in knowing. Regression also provides nice statistical properties including, but not limited to, statistical significance, confidence interval, &amp; standard error of each estimator (our coefficients of interest) which provides a CMO with upper/lower bound of estimates and level of confidence for our estimates. (From Marketing Analytics To Marketing Science I talk more about why we should look at marketing problems from more of a scientific lens) Swiss Army Knife of Data Science: We should also appreciate regression for its versatility and the wide variety of applications within marketing. Regression is primarily used for:  Predictive Analytics — regression is also a foundational piece for predictive analytics. This is what we’ve been talking about so far. We model the relationships between variables to make predictions about the value of the independent variable based on what we know for the dependent variables.  Causal Inference — extending beyond just modeling the relationship between variables, we can leverage regression to make causal inference. Meaning making causal claims between variables. Some applications in marketing include:  Geo experimentation Causal impact of marketing events including IRL / Offline Marketing Leading scoring Product marketing and upsell / cross-sell Media mix modeling and budget optimization ForecastingLearning Regression: I wish learning regression was as simple as doodling some dots and drawing a line through them but the reality is that it is a vastly deep and complicated subject. But there are good news!  There is a saying “Nothing in life worth having come easy. ” Therefore regression must be worth having.  You don’t need to master regression before applying it. It is one of those subjects that you build up knowledge over time. There are plenty of Ph. D. students in statistics still learning regression. I have a couple links below as a starting point. If you have any good resources, please share in the comments below as well for others! Introduction To Regression Model (Coursera) Penn State — Stats 501 Regression Methods Mastering ‘Metrics: The Path from Cause to Effect Regression Modeling Strategies — Frank Harrell (Textbook) Regression Modeling Strategies — Frank Harrell (Course) "
    }, {
    "id": 20,
    "url": "/from-marketing-analytics-to-marketing-science/",
    "title": "From Marketing Analytics To Marketing Science",
    "body": "2018/12/10 - “Half the money I spend on advertising is wasted; the trouble is I don’t know which half. ” The changes in the technology landscape in the past two decades have evolved the marketing industry as a whole, creating an entirely new field called digital marketing. First the dot com and internet boom, followed by an explosion of web data, and finally creation of cheap and readily available web analytics tools, digital marketers have much more insight into how their marketing campaigns are performing. Still which this much data and tools, marketers still have trouble answer this question: Web analytic tools like Google Analytics &amp; Adobe Omniture has made marketing analytics a commodity and a marketer’s best friend. With a click of few button, marketers have the ability to see how many clicks, impression and even conversion their marketing campaigns have generated. They can see how their campaigns stack up against each other and overtime, turning marketing into a analytics and metrics driven profession.  But since then, the move from purely analytics approach to more science-based has been slow. You probably now questioning what I mean by science vs analytics here so let’s take a quick moment to explain how I define the two: Analytics approach → Answering business questions by leveraging descriptive and predictive analytics  Looking at the data overtime and identifying trends. Are things doing better or worse? Segmenting / slicing &amp; dicing the data to look at it from different perspectives. Are certain segments doing better or worse? Making decisions based on correlation between variables. When X moves, Y seems to move as well, so let’s do more of X.  Using the data to make predictions in the future. Will this person churn?Science approach → interpreting data and drawing inference with scientific rigor  Attempting to find causality between variables, what’s causing what and at what magnitude Looking at data in terms of estimate, probability, confidence interval, posterior distribution etc. I’m not sure what X is but I’m fairly it’s between Y and Z Experimental design to prove/disprove hypothesis. Running randomized trial. One science approach that marketing has adopted is experimental design in the form of A/B testing. Although widely used within marketing (and other areas), I’d argue a lot of the designed experiments implemented in many corporations are questionable. This will be a separate post. I believe that marketing analytics technology has matured enough and as marketers and data analyst, we should look at marketing from a scientific lens in additional to traditional analytics. I’m Not Sure If I Understand The Difference: My explanation above might be too theoretical and high level. Let’s go through an example to better understand. Assume Sally is a marketer and she does SEM for Company X. She sets up a new campaign targeting Company X as the keyword. After a week she sees that her campaign received 100K impressions, 5000 clicks, 50 customers and spent a total of $500. The first question that needs to be answered is: “are the results good or bad”? How would Sally go about answering this question? A general approach would be benchmark her new campaign with existing campaigns, comparing CPC and CAC. Let say she compares across several campaigns and on average CPC is around $0. 20 and CAC is around $20, meaning her new campaign is 2x more efficient. This is great, seems like this campaign is killing it, let’s double down and spend more on this campaign. This is typically how marketing data is interpreted and actioned on. Marketers and data analyst looks at overall trends and make a decision based on insight drawn. So what’s the problem here? Well several key problems:  We never actually answered whether the campaign is performing well. We only looked at whether this campaign performed relatively well compared to historical aggregate of data What if all this campaign did was take from other campaigns? What is the probability that the results we saw happened by chance?What we should be interested in here instead is the incremental benefit of having this campaign to get a true sense of CAC. Sure the results might look great at face value but what if all it’s doing is taking away traffic from your Organic Search. Meaning these clicks would of been captured through Organic Search anyways. Actually in this case, the campaign is adding negative value since now Company X is paying for “free” traffic it would of gotten anyways. Ok, So How Can I Be More Scientific With Marketing?: This is just one example (though very common) of using marketing analytics to drive “insights”. As seen, the problem with pure analytics approach is that we might not be answering the true question of interest. At best we may be making decisions based on signals that are highly correlated with the truth, at worst we are making decisions that are purely noise. Imagine if your company has a nine figure marketing budget, every decision becomes very expensive and critical to get right.  So how can we answer this question? Methods and techniques to answer this question goes back to what this article is about, scientific approaches including experimental design and regression analysis can be used to help answer this question. I won’t go into too much detail in this introductory article (will in future articles) but designing a geo experiment or building a regression based model to understand the counterfactual are techniques that can be used. A Mindset Shift: Evolving from marketing analytics to marketing science first requires a shift in mindset. We need to understand that more data does not mean more data informed decisions. We need to look at each problem and insight and critic it with rigor to determine if we are simply making inference on correlation or causation. I’m not saying that analytics is not useful to make decisions, far from that. Analytics provide a way to draw quick “insights” (better than just throwing darts in the dark). What I’m highlighting is the need to complement analytics with more science to make sure we’re hitting bullseye. "
    }, {
    "id": 21,
    "url": "/vanderbilt-university-regression-modelling-strategies-course/",
    "title": "Vanderbilt Regression Modelling Strategies Course",
    "body": "2018/06/02 - I feel very fortunate to work for an employer like Shopify, that places a lot of value in continuous learning and self development. One of the many benefits that we get as employees is an annual self-development budget that we can use on anything to level-up ourselves professionally. This year I decided to experiment with taking a 5 day short course on Regression Modelling Strategies offered at Vanderbilt University. This blog entry is a summary of sed course material, my experience and recommendations for other Data Scientists in the industry that might be interested in diving deeper into the topic of regression. Inspiration: The most common way people tend to use their development budget is by attending conferences. I also know of some coworkers who have used their development budget over several years to pursue a formal University certification in Big Data / Data Science. Personally, I have mix feelings about both and I don’t find either one to fit my learning style very well. I’ve been to couple conferences related to data science and marketing but I felt I wasn’t able to take a whole lot away. Some felt way too product and sales focused while others lacked the content that I was looking for. The challenge that I have with some of these University Big Data certificate programs is the format of the education. Typically the format is a set of courses over several semesters and classes are mandatory on a weekly basis, with midterms, assignments, and exams. What I struggle with is the pace of the learning, I feel that it is too slow for me. I also dislike the breadth and lack of depth of the content. I feel like a lot of topics are usually covered in such programs but none are explored deeply. At this point of my career, I feel I have a better sense of what I know, what I don’t know, and what I want/need to know, which has consequently made me explore alternative options for my development budget. Browsing on Twitter ultimately led me to Frank Harrell’s RMS course. I was very intrigued by the concept of a condensed course that focused on a topic that I wanted to learn more about and thought was very relevant for my day to day work. Course Summary: The course I took is called Regression Modelling Strategies, taught by Professor Frank Harrell at Vanderbilt University. This is a condensed version of the course that he regularly teaches during the semester. The course is broken up into 1 day (optional) R tutorial and 4 full days of course content. The course is intended for Masters and PhD level students, so there is an expectation that students already have a competent understanding of multiple regression modelling. The course becomes very challenging to follow along if students do not have that understanding or are not prepared for the lectures. For more information about the course, see here. The philosophy of the course and Professor Frank Harrell can be summarized as:  It is more productive to make model fit step by step than to postulate a simple model and find out what went wrong Carefully fitting improper model is better than badly fitting a well chosen one A good overall strategy is to decide how many degrees of freedom (regression parameters) can be “spent”, where they should be spent, and to spend them with no regretsTopics covered in the course include but not limited to the following:  Hypothesis Testing vs Estimation vs. Prediction vs. Classification General Aspects of Fitting Regression Models     Interpreting Model Parameters &amp; Understanding Interactions   Relaxing Linearity Assumptions for Continuous Predictors         Non Linear Terms     Splines for Estimating Shape of Regression Function     Cubic / Restricted Cubic Splines / Choosing Number and Position of Knots     Nonparametric Regression     Assessment of Model Fit           Handling Missing Data     Strategies for Developing Imputation Model   Predictive Mean Matching    Multivariable Modelling Strategies     Variable Selection, Sample Size, Overfitting, Limit on Number of Predictors   Shrinkage   Data Reduction Techniques    Describing, Resampling, Validating and Simplifying the Model     Model Validation   Bootstrap vs. Cross Validation    Binary Logistic Regression, Ordinal Logistic Regression, Survival Analysis Case StudiesFor a more detailed list of topics, click here. Who Should Attend This Course?: The course description illustrates the target audience as “statisticians and related quantitative researchers who want to learn some general model development strategies, including approaches to missing data imputation, data reduction, model validation, and relaxing linearity assumptions”. In my opinion, I feel this topic / course is suitable for anyone who is working with data, primarily focusing on problems that require a good understanding of how variables interact with each other and affect outcome. My domain of interest is marketing and I am interested in answering questions such as, what levers affect churn or LTV, and at what magnitude? How does each marketing channel influence customer conversion? This is a traditional statistics course, with a focus on biostatistics, and not a machine learning / data science course. I don’t recommend this for anyone that is only interested in the prediction aspect of modelling and not the interpretation aspect. Although this is a biostatistics course, I think the content is relevant for people outside of the medical field as well. My class comprised of primarily doctors, postgraduate students, researchers or people from the medical industry, but there were also people from the finance sector, geology, etc. In my opinion, I actually think the biostatistics aspect of the course is beneficial and makes the course much more interesting. Surrounding yourself with people that are not in your field and looking at problems that you don’t look at on a day to day basis helps stimulate thoughts and creativity. Recommendations: Overall I highly recommend other people in the industry to take this course if they are interested in learning more about regression modelling and also have similar reservations towards conferences and long form education. I think there are couple things that can really help enhance this experience (from the perspective of someone coming from industry and not academia):  Going as a team (4-5 people): Not only is this option more cost efficient, I think you can learn a lot more when you are studying as a group, by asking each other questions, sharing ideas and thoughts, challenging ideas etc.  Being very prepared for the course so that you can have meaningful conversations with Professor Frank Harrell. There were several students that were taking the course for the 2nd or 3rd time and I noticed that they were able to have a deep conversations with the Professor about specific problems they are working on. There is a textbook that comes with this course, and I would highly encourage going over that material before attending the course as well during the course.  Course improvements mentioned below. Improvements: The course is not perfect and there are definitely improvements that can be made. I’ve provided the same feedback on the course feedback survey.  The R tutorial is not necessary. I would prefer the R tutorial to be removed and allot an extra day for the course. The R content can be sent out prior to the course for students to review individually There is a lack of interaction / collaboration amongst the students. I think having students from very diverse fields in one place is in itself a very valuable. The concept of having a mini group project intrigues me. Replacing the R tutorial day with extra half day of course content and half day of project presentation is also an intriguing idea Go deeper into survival analysis and ordinal regression Price for non institute members is fairly expensive and does not include airfare and accomodations, so that is something to considerTakeaway: I am very happy that I experimented with using my development budget on a short course. One thing to keep in mind is that you should not make the assumption that after a 5 day course you will know everything about regression modelling. That is simply impossible.  You will need to continue to self study but the course provides you with the foundation to continue exploring this topic further. I am currently still reading/re-reading the course textbook and course notes. At the same time I am reading academic papers that apply regression in the realm of marketing and I find that I am able to understand the methodologies behind the modelling process. If you have any thoughts/comments/questions about my experience, feel free to reach out over Twitter or email. I would love to help address any questions or further share with you my experience. If you know of any other good short courses, please please please reach out. I would love to learn more about it! "
    }, {
    "id": 22,
    "url": "/marvel-cinematic-plotly/",
    "title": "Marvel Cinematic Data Visualization With Plot.ly",
    "body": "2017/06/01 - I’m a huge sucker for Marvel cinematic and in this article I will do a fun exercise with building a simple interactive 3D network graph based on the relationship between Marvel characters. I will be using one of my favourite plotting libraries in Python, Plot. ly. Plot. ly is very easy to use and the way graphs are constructed is very intuitive. The dataset can be found on my GitHub or at the following link: https://www. kaggle. com/csanhueza/the-marvel-universe-social-network Setting up and exploring the data: import plotly. plotly as pyfrom plotly. graph_objs import *import plotly. offline as offlineimport pandas as pdKey thing to remember with Plot. ly is if you want to build graphs locally on you computer using Jupyter notebooks, you need to initiate offline notebook mode offline. init_notebook_mode()The hero-network. csv dataset contains two columns, hero1 and hero2 and represents a connection between the two characters. heros = pd. read_csv('hero-network. csv')heros. describe()         hero1   hero2         count   574467   574467       unique   6211   6173       top   CAPTAIN AMERICA   CAPTAIN AMERICA       freq   8149   8350   Some quick data cleaning to remove empty spaces for i in range(0,2):  heros[heros. columns[i]] = heros[heros. columns[i]]. map(lambda x: x. rstrip())If you are playing around with the same code and want to explore additional characters, you can use the following line to explore what type of characters are included in the dataset #heros[heros['hero1']. str. contains('DAREDEVIL')]['hero1']. unique()For this exercise, I’m interested in looking at the connection between some of the marvel characters and villains that we’ve see in theatres! avengers_name = ['HULK/DR. ROBERT BRUC', 'BLACK WIDOW/NATASHA', 'CAPTAIN AMERICA', 'IRON MAN/TONY STARK',         'WAR MACHINE II/PARNE', 'HAWKEYE | MUTANT X-V', 'FALCON/SAM WILSON',         'SCARLET WITCH/WANDA', 'VISION', 'ANT-MAN II/SCOTT HAR', 'SPIDER-MAN/PETER PAR',          BLACK PANTHER/T'CHAL , 'DR. STRANGE/STEPHEN', 'THOR IV/DARGO', 'FURY, COL. NICHOLAS',         'QUICKSILVER/PIETRO M'        ]villain_name = [ 'BUCKY/BUCKY BARNES', 'MALEKITH/MALCOLM KEI', 'THANOS', 'ULTRON',         'LOKI [ASGARDIAN]', 'BARON MORDO/KARL MOR', 'DORMAMMU', 'RED SKULL/JOHANN SCH']all = []all. extend(avengers_name)all. extend(villain_name)First lets just explore The Avenger’s social circle: We do a quick count on how many relationships each avengers have and how many comic books they appeared in avenger_info = {'hero': [], 'buddies': []}for i in avengers_name:  avenger_info['hero']. append(i)  avenger_info['buddies']. append(len(heros[heros['hero1']==i]))avenger_df = pd. DataFrame(avenger_info, columns = ['hero', 'buddies', 'comics'])Building a grouped bar chart in Plotly is very simple. Each set of bar is treated as seperate data, we define the x and y values and the aesthetics for each group of bars. Then we define the layout like axes, chart title etc. Lastly we pass data and layout into Plot. ly’s figure function to build the graph buddies = Bar(  x = avenger_df['hero'],  y = avenger_df['buddies'],  name = 'buddies',  marker=dict(    color='rgba(234, 35, 40, 0. 7)',    line=dict(      color='rgba(234, 35, 40, 1. 0)',      width=1    )  ))data = [buddies]layout = Layout(  barmode = 'group',  bargroupgap=0. 1,  title = 'Avengers - Buddies')fig = Figure(data = data, layout = layout)offline. iplot(fig)We can see our fellow Captain Steve Rogers is quite popular along with friendly neighborhood Spiderman and Mr. Tony Stark. One weird data point here is Thor who in my mind is should be quite popular… This might be because there are several Thor characters in the dataset, representing different Thors from different universes, as well as the comic book universe being different from cinematics. Now lets build the network graph for our Avengers and villains: First off, there are a whole lot of characters in our dataset, lets just keep our avengers and villains def keep_avengers(x, characters):  if ((x['hero1'] in characters) and (x['hero2'] in characters)):    return 1  else:    return 0heros['keep'] = heros. apply(lambda x: keep_avengers(x, all), axis=1)heros = heros[heros['keep']==1]. drop('keep', axis=1). reset_index(). drop('index', axis=1)Next we use the igraph libary which is a library for high-performance graph generation and analysis. For more information on installation, visit http://igraph. org/python/ import igraph as igOne of the requirement to build this network graph is to express the source and destination nodes as integer values. So we start off with encoding our heros into numbers heros. head(3)         hero1   hero2         0   SPIDER-MAN/PETER PAR   HULK/DR. ROBERT BRUC       1   QUICKSILVER/PIETRO M   SCARLET WITCH/WANDA       2   QUICKSILVER/PIETRO M   IRON MAN/TONY STARK   Edges = []mapper = {}character_group = []unique_heros = pd. concat([heros['hero1'], heros['hero2']]). unique()num_unique_heros = len(unique_heros)for i in range(num_unique_heros):  mapper[unique_heros[i]] = iheros['hero1_node'] = heros['hero1']. map(lambda x: mapper[x])heros['hero2_node'] = heros['hero2']. map(lambda x: mapper[x])heros. head(3)         hero1   hero2   hero1_node   hero2_node         0   SPIDER-MAN/PETER PAR   HULK/DR. ROBERT BRUC   0   3       1   QUICKSILVER/PIETRO M   SCARLET WITCH/WANDA   1   4       2   QUICKSILVER/PIETRO M   IRON MAN/TONY STARK   1   2   Next I’d like to seperate our good guys from the bad guys visually, so let’s group them up. for j in unique_heros:  character_group. append(1) if (j in avengers_name) else character_group. append(0)Now were ready to build our nodes and links. We start off with links by putting our (source, destination) for every node into Edges and pass Edges into our igraph. The layout function here defines the overall structure of our network graph and we use ‘sphere’. There are other options like    gfr, grid_fr, grid_fruchterman_reingold: grid-based Fruchterman-Reingold layout     kk, kamada_kawai: Kamada-Kawai layout     kk_3d, kk3d, kamada_kawai_3d: 3D Kamada-Kawai layout  Play around with some of these to see the different structures. for i in range(len(heros)):  Edges. append((heros['hero1_node'][i], heros['hero2_node'][i]))G = ig. Graph(Edges, directed=False)layt=G. layout('sphere', dim=3)This part looks a little intimidating and complicated but its not so bad. Layout function helps us define the layout of the network graph, what we are doing here is populating our X, Y, Z coordinates for each node and edge to be placed into our 3D space Xn=[layt[k][0] for k in range(num_unique_heros)]# x-coordinates of nodesYn=[layt[k][1] for k in range(num_unique_heros)]# y-coordinatesZn=[layt[k][2] for k in range(num_unique_heros)]# z-coordinatesXe=[]Ye=[]Ze=[]for e in Edges:  Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends  Ye+=[layt[e[0]][1],layt[e[1]][1], None]  Ze+=[layt[e[0]][2],layt[e[1]][2], None]Same as how we built the bar chart, we define our data &amp; layout and pass into the Figure function. Instead of bar chart, we are using Scatter3d for our data trace1=Scatter3d(x=Xe,        y=Ye,        z=Ze,        mode='lines',        line=Line(color='rgb(125,125,125)', width=1),        hoverinfo='none'           )trace2=Scatter3d(x=Xn,        y=Yn,        z=Zn,        mode='markers',        name='actors',        marker=Marker(symbol='dot',               size=13,               color=character_group,               colorscale=[                [0, 'black'],                [1, 'red']],               line=Line(color='rgb(50,50,50)', width=0. 5),               opacity = 0. 8               ),        text=unique_heros,        hoverinfo='text'       )axis=dict(showbackground=False,     showline=False,     zeroline=False,     showgrid=False,     showticklabels=False,     title='',     showspikes = False     )layout = Layout(     title= Marvel Cinematic - Social Network ,     width=1000,     height=700,     showlegend=False,     scene=Scene(     xaxis=XAxis(axis),     yaxis=YAxis(axis),     zaxis=ZAxis(axis)    ),    hovermode='closest'  )data=Data([trace1, trace2])fig=Figure(data=data, layout=layout)offline. iplot(fig)"
    }, {
    "id": 23,
    "url": "/dimension-reduction-pca/",
    "title": "Dimension Reduction with Principal Component Analysis",
    "body": "2017/05/01 - PCA is one of the most popular techniques for dimensionality reduction. If you have no idea what I mean by dimensionality reduction, check out part 1 of this topic. In this article, we’ll explore PCA with a more applied approach rather than mathematical and we’ll keep certain details in a blackbox for future discussions. What is PCA? PCA is an algorithm that transforms a dataset to lower dimensional dataset. The keyword here is transform which is different from feature selection that I’ve talked about in part 1. The difference being that features are not removed from the dataset but instead the dataset itself is transformed. Simple hypothetical example: We have on the left the original dataset comprising of length, width, and height in centimetres. If we were to apply PCA to reduce this down to two dimensional dataset, we would get something like the right where we have two features, let say, Z1 and Z2. Z1 and Z2 is not produced from removing one of the features but instead all three features (L, W, H) are transformed and Z1 and Z2 does not represent the same measurements (cm) anymore. The goal of this algorithm is to reduce the dimensions of the dataset while retaining majority of the information, meaning maximizing the amount of variance kept. Another way of interpreting this is that the algorithm looks for vector(s) when projecting the data that minimizes the overall projection error. Lets take a look at what this means: On the left we couple data points in 2D space that we hope to use PCA to reduce down to a one dimension vector. On the top right, we project the data onto the solid line. The dotted lines from the data point to the solid line is what we call the projection error. On the bottom right is another way of projecting the data onto another line. It is evident that the projection error of the bottom right graph is much bigger than the projection error of the top right. What is not as evident is that the variance of the projected data points on the line is much bigger for the top right graph vs. the bottom right. Whether it is to minimize projection error or maximize variance, PCA’s goal is to retain as much information about the dataset as possible. For this reason, one important step before applying this algorithm is to perform normalization on the mean and variance for each feature and scaling on the range. A feature (before scaling) with a disproportionately large variance will be tend to be favoured in PCA because to the algorithm this feature explains majority of the variance in the original dataset. Now that you have a high level conceptual understanding of PCA, checkout the notebook belong on practical examples of how to apply PCA with sci-kit learn! I will illustrate two examples, one to highlight visually the results of PCA on a simple 2D &amp; 3D dataset and another applying PCA on a high dimensional dataset. For the high dimensional datset, I will use the same dataset from Kaggle about food nutrients that I’ve used previously. https://www. kaggle. com/openfoodfacts/world-food-factsThis data set has 134754 rows and 161 columns. One row per food product. import pandas as pdimport numpy as npfrom sklearn import datasetsfrom sklearn. decomposition import PCAimport matplotlib. pyplot as plt%matplotlib inlinefrom sklearn import preprocessingDefined # of Principal Components: We use the iris dataset from sklearn to demonstrate visually how to apply PCA on a 2D dataset and reduce to 1D and a 3D dataset into 2D. iris = datasets. load_iris()Before applying PCA, we need to normalize our data to mean of 0 and unit variance. We can use the scale function in the preprocessing module from sklearn. X_2d = iris. data[:, :2] X_2d_scale = preprocessing. scale(X_2d)print  Mean:   + str(X_2d_scale. mean(axis=0)). strip('[]')print  Standard Deviation:   + str(X_2d_scale. std(axis=0)). strip('[]')Mean: -1. 69031455e-15 -1. 63702385e-15Standard Deviation: 1.  1. Plotting the original and scaled data set. The two plot looks very similar because the range and variance of Sepal Length and Sepal Width are naturally similar. fig = plt. figure()fig. set_size_inches(15, 5)ax1= fig. add_subplot(121)ax1. scatter(X_2d[:, 0], X_2d[:, 1])ax1. set_xlabel('Sepal Length - Original')ax1. set_ylabel('Sepal Width - Original')ax2= fig. add_subplot(122)ax2. scatter(X_2d_scale[:,0], X_2d_scale[:,1])ax2. set_xlabel('Sepal Length - Scaled')ax2. set_ylabel('Sepal Width - Scaled')fig. suptitle('Sepal Length vs. Sepal Width (Original &amp; Scaled)') We want to reduce this dataset from 2D to 1D, using the PCA module from sklearn we specify to keep 1 component. pca = PCA(n_components = 1)pca_2d = pca. fit_transform(X_2d)To demonstrate more visually the results of PCA, lets take a look at reducing 3D dataset to 2D. Like previously, we first normalize our dataset X_3d = iris. data[:, :3] X_3d_scale = preprocessing. scale(X_3d)print  Mean:   + str(X_3d_scale. mean(axis=0)). strip('[]')print  Standard Deviation:   + str(X_3d_scale. std(axis=0)). strip('[]')Mean: -1. 69031455e-15 -1. 63702385e-15 -1. 48251781e-15Standard Deviation: 1.  1.  1. from mpl_toolkits. mplot3d import Axes3Dfig = plt. figure()fig. set_size_inches(15, 10)ax1 = fig. add_subplot(121, projection='3d')ax1. scatter(X_3d_scale[:, 0], X_3d_scale[:, 1], X_3d_scale[:, 2])ax1. set_xlabel('Sepal Length - Scaled')ax1. set_ylabel('Sepal Width - Scaled')ax1. set_zlabel('Petal Length - Scaled')ax2 = fig. add_subplot(122, projection='3d')ax2. scatter(X_3d_scale[:, 0], X_3d_scale[:, 1], X_3d_scale[:, 2])ax2. view_init(elev=0)ax2. set_xlabel('Sepal Length - Scaled')ax2. set_ylabel('Sepal Width - Scaled')ax2. set_zlabel('Petal Length - Scaled')fig. suptitle('Sepal Length vs Sepal Width vs Petal Length - Scaled') Specify to keep 2 principal components. After PCA, our dataset no longer represent Sepal Width, Sepal Length and Petal Length as PCA has transformed and projected our dataset into an arbitrary space, let say Z1, Z2, Z3 pca = PCA(n_components = 2)pca_3d = pca. fit_transform(X_3d_scale)fig = plt. figure()fig. set_size_inches(15, 10)ax1= fig. add_subplot(121, projection='3d')ax1. scatter(pca_3d[:, 0], pca_3d[:, 1], 0)ax1. set_xlabel('Z1')ax1. set_ylabel('Z2')ax1. set_zlabel('Z3')ax2= fig. add_subplot(122, projection='3d')ax2. scatter(pca_3d[:, 0], pca_3d[:, 1], 0)ax2. view_init(elev=0)ax2. set_xlabel('Z1')ax2. set_ylabel('Z2')ax2. set_zlabel('Z3') Solving For Number Of Principal Components: Whats powerful about sklearn is that the same modules we used previously for PCA, under the circumstances that we don’t know the number of principal components prior to applying the reduction, can be used to solve for the number of principal components required to keep a certain % of variance within the dataset. Typically we want to retain 90%, 95% or 99% of the variance but depends on use case. Lets use the same iris data as an example first then the food nutrient dataset from Kaggle. iris_3d = X_3d_scaleTo find the number of principal components, in the same PCA function, we define n_components to be 0 &lt; n_components &lt; 1 which is the % of variance that we want to keep in the dataset and we specify svd_solver == ‘full’. Lets start with 95% then 30% and see what happens p_95 = PCA(n_components = 0. 95, svd_solver='full')p_95. fit(iris_3d)PCA(copy=True, iterated_power='auto', n_components=0. 95, random_state=None, svd_solver='full', tol=0. 0, whiten=False)print  We see that to keep 95% of the variance in the original dataset, we can reduce the dataset into 2D \n print  The number of components :  + str(p_95. n_components_)print  The % of variance explained by each component:   + str(p_95. explained_variance_ratio_)We see that to keep 95% of the variance in the original dataset, we can reduce the dataset into 2D The number of components :2The % of variance explained by each component: [ 0. 67127544 0. 30494357]p_30 = PCA(n_components = 0. 30, svd_solver='full')p_30. fit(iris_3d)PCA(copy=True, iterated_power='auto', n_components=0. 3, random_state=None, svd_solver='full', tol=0. 0, whiten=False)print  We see that to keep only 30% of the variance in the original dataset, we can reduce the dataset into 1D \n print  The number of components :  + str(p_30. n_components_)print  The % of variance explained by each component:   + str(p_30. explained_variance_ratio_)We see that to keep only 30% of the variance in the original dataset, we can reduce the dataset into 1D The number of components :1The % of variance explained by each component: [ 0. 67127544]Applying PCA on food nutrient dataset from Kaggle: If you haven’t already, check out part 1 of dimensionality reduction where I’ve applied simple feature selection methods to the same dataset. food_data = pd. read_csv('openfoodfacts. tsv', sep='\t')We will do some preprocessing work on this dataset for this example:  dataset contains categorical variables, we will only explore the numerical variables for now there are variables with no data, we will remove these variables apply normalization apply feature scaling because different nutrients have different units and range of valuesfood = food_data. copy()numeric_columns = food. dtypes[food. dtypes == 'float64']. index #Extract numerical variablesfood = food[numeric_columns]missing = food. isnull(). sum() #Count number of missing valuespct_missing = 1. 0*missing/len(food) #Calculate percentagefood = food[pct_missing[pct_missing != 1. 0]. index] #Remove variables that have no datafood. shape(134754, 89)for i in food. columns:  food[i]. fillna(value=food[i]. mean(), inplace=True) #replace NaN with mean of dimension  food[i] = preprocessing. scale(food[i]) #normalization  food[i] = preprocessing. MinMaxScaler(). fit_transform(food[i]. values. reshape(-1,1)) #scale min max to 0-1There are 89 variables that we are exploring now in this processed dataset. How many do we keep? Let’s define that we want to keep 95% of the variance p = PCA(n_components = 0. 95, svd_solver='full')p. fit(food)PCA(copy=True, iterated_power='auto', n_components=0. 95, random_state=None, svd_solver='full', tol=0. 0, whiten=False)p. n_components_11p. explained_variance_ratio_array([ 0. 46515374, 0. 19124263, 0. 09001465, 0. 0692121 , 0. 04144684,    0. 03047795, 0. 01878442, 0. 0172208 , 0. 01577135, 0. 00958264,    0. 0090774 ])sum(p. explained_variance_ratio_)0. 95798452774543918As seen, we can reduce the 89 variables down to 11 components to keep 95% of the variance. The explain_variance_ratio provides information on how much variance is kept for each component. The sum of it we can see is &gt;95% which is the minimal we’ve defined. If we want more variance to be kept then # of components will increase and vice versa! "
    }, {
    "id": 24,
    "url": "/dimension-reduction-correlation-low-variance-filter/",
    "title": "Dimension Reduction Correlation & Low Variance Filter",
    "body": "2017/04/27 - What is dimensionality reduction? The name might sound fancier than what it actually is. It is simply the process of reducing the number of dimensions in a dataset aka reducing the number of attributes/features/columns while retaining key information in the dataset.  Wait, but why?  Because more data does not necessarily mean we get better model performance. Many attributes could be noise to the key signals in a dataset (overfitting, curse of dimensionality).  Data compression to reduce storage size, which reduces computational resource and helps speed up algorithm.  Reducing data to 2D or 3D allows us to visualize the data. There are two approaches to dimensionality reduction comprising different techniques/algorithms:  Feature Selection – selecting subset of feature in the dataset without transforming the dataset as a whole Feature Extraction – transforming the dataset into a lower dimensional spaceFor part 1 of dimensionality reduction, we’ll get started with applying three simple feature selection techniques using Python. import pandas as pdimport numpy as npTo explore how to apply different dimension reduction techniques in Python, I will use a data set on food nutrient facts from Kaggle as an example. This data set has 134754 rows and 161 columns. One row per food product. https://www. kaggle. com/openfoodfacts/world-food-facts food_data = pd. read_csv('openfoodfacts. tsv', sep='\t')food_data. shape(134754, 161)food_data. columnsIndex([u'code', u'url', u'creator', u'created_t', u'created_datetime',    u'last_modified_t', u'last_modified_datetime', u'product_name',    u'generic_name', u'quantity',    . . .    u'ph_100g', u'fruits-vegetables-nuts_100g',    u'collagen-meat-protein-ratio_100g', u'cocoa_100g', u'chlorophyl_100g',    u'carbon-footprint_100g', u'nutrition-score-fr_100g',    u'nutrition-score-uk_100g', u'glycemic-index_100g',    u'water-hardness_100g'],    dtype='object', length=161)food_data. head(4)         code   url   creator   created_t   created_datetime   last_modified_t   last_modified_datetime   product_name   generic_name   quantity   . . .    ph_100g   fruits-vegetables-nuts_100g   collagen-meat-protein-ratio_100g   cocoa_100g   chlorophyl_100g   carbon-footprint_100g   nutrition-score-fr_100g   nutrition-score-uk_100g   glycemic-index_100g   water-hardness_100g         0   3087   http://world-en. openfoodfacts. org/product/0000. . .    openfoodfacts-contributors   1474103866   2016-09-17T09:17:46Z   1474103893   2016-09-17T09:18:13Z   Farine de blé noir   NaN   1kg   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       1   24600   http://world-en. openfoodfacts. org/product/0000. . .    date-limite-app   1434530704   2015-06-17T08:45:04Z   1434535914   2015-06-17T10:11:54Z   Filet de bœuf   NaN   2. 46 kg   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       2   27083   http://world-en. openfoodfacts. org/product/0000. . .    canieatthis-app   1472223782   2016-08-26T15:03:02Z   1472223782   2016-08-26T15:03:02Z   Marks % Spencer 2 Blueberry Muffins   NaN   230g   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       3   27205   http://world-en. openfoodfacts. org/product/0000. . .    tacinte   1458238630   2016-03-17T18:17:10Z   1458238638   2016-03-17T18:17:18Z   NaN   NaN   NaN   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4 rows × 161 columns food_data. isnull(). sum()code                  23url                   23creator                 2created_t                4created_datetime            10last_modified_t             0last_modified_datetime          0product_name             18578generic_name             84411quantity               35948packaging              59313packaging_tags            59311brands                27194brands_tags             27200categories              56286categories_tags           56307categories_en            56286origins               113257origins_tags            113294manufacturing_places        100295manufacturing_places_tags      100301labels                93449labels_tags             93363labels_en              93342emb_codes              106460emb_codes_tags           106463first_packaging_code_geo      116621cities               134732cities_tags             115136purchase_places           79454                   . . .  biotin_100g             134437pantothenic-acid_100g        134074silica_100g             134717bicarbonate_100g          134676potassium_100g           134058chloride_100g            134601calcium_100g            130445phosphorus_100g           133870iron_100g              132149magnesium_100g           133475zinc_100g              134271copper_100g             134598manganese_100g           134609fluoride_100g            134676selenium_100g            134575chromium_100g            134735molybdenum_100g           134744iodine_100g             134499caffeine_100g            134705taurine_100g            134720ph_100g               134706fruits-vegetables-nuts_100g     133091collagen-meat-protein-ratio_100g  134591cocoa_100g             133904chlorophyl_100g           134754carbon-footprint_100g        134489nutrition-score-fr_100g       67502nutrition-score-uk_100g       67502glycemic-index_100g         134754water-hardness_100g         134754dtype: int64Missing Value Ratio: Attributes with a lot of missing values are not providing a lot of information. We can either impute the values for these attributes or remove from dataset. We compute the % of missing values and determine whether to drop the attribute or not. The threshold is up to you but roughly speaking an attribute with ~40-50% missing values could be dropped. We can leverage isnull function from pandas dataframe to count how many empty fields are in each column. We can also leverage this information to calculate the % of missing values for each attribute. missing = food_data. isnull(). sum()pct_missing = 1. 0*missing/len(food_data)print pct_missing. sort_values(ascending=False)water-hardness_100g           1. 000000-nervonic-acid_100g           1. 000000no_nutriments              1. 000000ingredients_from_palm_oil        1. 000000ingredients_that_may_be_from_palm_oil  1. 000000nutrition_grade_uk            1. 000000-butyric-acid_100g            1. 000000-caproic-acid_100g            1. 000000-lignoceric-acid_100g          1. 000000-cerotic-acid_100g            1. 000000glycemic-index_100g           1. 000000-elaidic-acid_100g            1. 000000-mead-acid_100g             1. 000000-erucic-acid_100g            1. 000000-melissic-acid_100g           1. 000000chlorophyl_100g             1. 000000-myristic-acid_100g           0. 999993-caprylic-acid_100g           0. 999993-montanic-acid_100g           0. 999993-stearic-acid_100g            0. 999993-palmitic-acid_100g           0. 999993-capric-acid_100g            0. 999985-lauric-acid_100g            0. 999970-maltose_100g              0. 999970nucleotides_100g             0. 999948-arachidonic-acid_100g          0. 999941molybdenum_100g             0. 999926-maltodextrins_100g           0. 999918-oleic-acid_100g             0. 999911serum-proteins_100g           0. 999896                      . . .  proteins_100g              0. 445916energy_100g               0. 440907packaging                0. 440158packaging_tags              0. 440143image_url                0. 434577image_small_url             0. 434577main_category              0. 418021main_category_en             0. 418021categories_tags             0. 417850categories                0. 417694categories_en              0. 417694pnns_groups_1              0. 372085pnns_groups_2              0. 351440quantity                 0. 266768brands_tags               0. 201849brands                  0. 201805product_name               0. 137866countries_tags              0. 002070countries                0. 002070countries_en               0. 002070states_en                0. 000341states_tags               0. 000341states                  0. 000341url                   0. 000171code                   0. 000171created_datetime             0. 000074created_t                0. 000030creator                 0. 000015last_modified_datetime          0. 000000last_modified_t             0. 000000dtype: float64It is evident there are significant number of attributes that barely have any information. Lets remove features that have less than 25% value. new_food_data = food_data[pct_missing[pct_missing &lt; 0. 75]. index. tolist()]new_food_data. columnsIndex([u'code', u'url', u'creator', u'created_t', u'created_datetime',    u'last_modified_t', u'last_modified_datetime', u'product_name',    u'generic_name', u'quantity', u'packaging', u'packaging_tags',    u'brands', u'brands_tags', u'categories', u'categories_tags',    u'categories_en', u'manufacturing_places', u'manufacturing_places_tags',    u'labels', u'labels_tags', u'labels_en', u'purchase_places', u'stores',    u'countries', u'countries_tags', u'countries_en', u'ingredients_text',    u'serving_size', u'additives_n', u'additives', u'additives_tags',    u'additives_en', u'ingredients_from_palm_oil_n',    u'ingredients_that_may_be_from_palm_oil_n', u'nutrition_grade_fr',    u'pnns_groups_1', u'pnns_groups_2', u'states', u'states_tags',    u'states_en', u'main_category', u'main_category_en', u'image_url',    u'image_small_url', u'energy_100g', u'fat_100g', u'saturated-fat_100g',    u'carbohydrates_100g', u'sugars_100g', u'fiber_100g', u'proteins_100g',    u'salt_100g', u'sodium_100g', u'nutrition-score-fr_100g',    u'nutrition-score-uk_100g'],   dtype='object')Low Variance Filter: Attributes with very little change in its data, e. g. all values are 1s, also provides very little information. Similar to Missing Value Ratio, we remove attributes based on a define threshold of variance. Variance is range dependent therefore normalization is required and only applicable to numerical attributes. We need to normalize each dimension as variance is range dependent. We can use the MinMaxScaler function from sklearn preprocessing module to normalize value in each dimension to a value between 0 and 1. The challenge with this is that sklearn estimators does not handle NaN or missing values. An intermediate step is required to infer missing data with either mean or median or whatever statistics that would make most sense. There are different ways to do this like using fillna() function in pandas or Imputer module from sklearn. Another method we can simple define our own normalization function. from sklearn import preprocessingvar_fil_food_data = new_food_data. copy()scaler = preprocessing. MinMaxScaler()#Extract numeric columns because cannot normalize and compute variance on categoricalnumeric_columns = var_fil_food_data. dtypes[var_fil_food_data. dtypes == 'float64']. indexfor i in numeric_columns:  var_fil_food_data[i]. fillna(value=var_fil_food_data[i]. mean(), inplace=True) #replace NaN with mean of dimension  var_fil_food_data[i] = scaler. fit_transform(var_fil_food_data[i]. values. reshape(-1,1))   #Normalize. if we don't use . values. reshapes it still works but sklearn throws depracated warningvar_fil_food_data[numeric_columns]. mean()additives_n                0. 054490ingredients_from_palm_oil_n        0. 031198ingredients_that_may_be_from_palm_oil_n  0. 020675energy_100g                0. 012728fat_100g                  0. 133428saturated-fat_100g             0. 054690carbohydrates_100g             0. 072374sugars_100g                0. 130803fiber_100g                 0. 027809proteins_100g               0. 075218salt_100g                 0. 001004sodium_100g                0. 001005nutrition-score-fr_100g          0. 427915nutrition-score-uk_100g          0. 457145dtype: float64var_fil_food_data[numeric_columns]. var()additives_n                0. 003423ingredients_from_palm_oil_n        0. 008250ingredients_that_may_be_from_palm_oil_n  0. 002640energy_100g                0. 000061fat_100g                  0. 014988saturated-fat_100g             0. 003620carbohydrates_100g             0. 002530sugars_100g                0. 017339fiber_100g                 0. 000658proteins_100g               0. 003247salt_100g                 0. 000020sodium_100g                0. 000020nutrition-score-fr_100g          0. 013776nutrition-score-uk_100g          0. 017170dtype: float64Looking at the mean and variance, we could explore removing energy_100g, salt_100g sodium_100g. new_food_data = new_food_data. drop(['energy_100g', 'salt_100g', 'sodium_100g'], axis=1)Correlation Filter: Attributes that are highly correlated tends to carry similar information, e. g. a company’s overall spend and its marketing spend. Because highly correlated attributes contain similar information, we can keep just one of these attributes. To keep this example simple, we will only look at the correlation between numeric variables. For categorical variables, there is an additional encoding step (covered in another blog article) that is required, which simply splits every categorical value of one dimension into individual columns with binary values of 1 or 0. We can build a correlation matrix using the corr function in pandas. We could also use a more visual approach by using heatmap from seaborn library. corr_fil_food_data = new_food_data. copy()#Extract numeric columns because cannot normalize and compute variance on categoricalnumeric_columns = corr_fil_food_data. dtypes[corr_fil_food_data. dtypes == 'float64']. indexcorr_fil_food_data = corr_fil_food_data[numeric_columns]corr_fil_food_data. corr()         additives_n   ingredients_from_palm_oil_n   ingredients_that_may_be_from_palm_oil_n   fat_100g   saturated-fat_100g   carbohydrates_100g   sugars_100g   fiber_100g   proteins_100g   nutrition-score-fr_100g   nutrition-score-uk_100g         additives_n   1. 000000   0. 247840   0. 433042   -0. 027467   -0. 019006   0. 119767   0. 124980   -0. 107708   -0. 083062   0. 202419   0. 187053       ingredients_from_palm_oil_n   0. 247840   1. 000000   0. 179777   0. 108486   0. 142192   0. 211748   0. 168584   0. 011094   -0. 036060   0. 245711   0. 248023       ingredients_that_may_be_from_palm_oil_n   0. 433042   0. 179777   1. 000000   0. 042765   0. 044454   0. 122512   0. 052955   -0. 038976   -0. 058680   0. 121939   0. 125042       fat_100g   -0. 027467   0. 108486   0. 042765   1. 000000   0. 735497   -0. 071676   0. 023426   0. 082148   0. 146350   0. 591396   0. 655143       saturated-fat_100g   -0. 019006   0. 142192   0. 044454   0. 735497   1. 000000   -0. 012336   0. 121237   0. 020530   0. 131305   0. 623594   0. 664247       carbohydrates_100g   0. 119767   0. 211748   0. 122512   -0. 071676   -0. 012336   1. 000000   0. 637138   0. 246810   -0. 103593   0. 257640   0. 248387       sugars_100g   0. 124980   0. 168584   0. 052955   0. 023426   0. 121237   0. 637138   1. 000000   0. 034637   -0. 237634   0. 480360   0. 448149       fiber_100g   -0. 107708   0. 011094   -0. 038976   0. 082148   0. 020530   0. 246810   0. 034637   1. 000000   0. 230218   -0. 102295   -0. 092460       proteins_100g   -0. 083062   -0. 036060   -0. 058680   0. 146350   0. 131305   -0. 103593   -0. 237634   0. 230218   1. 000000   0. 094913   0. 156746       nutrition-score-fr_100g   0. 202419   0. 245711   0. 121939   0. 591396   0. 623594   0. 257640   0. 480360   -0. 102295   0. 094913   1. 000000   0. 967227       nutrition-score-uk_100g   0. 187053   0. 248023   0. 125042   0. 655143   0. 664247   0. 248387   0. 448149   -0. 092460   0. 156746   0. 967227   1. 000000   import seaborn as snsimport matplotlib. pyplot as plt%matplotlib inlinesns. set()fig, ax = plt. subplots()fig. set_size_inches(15, 15)sns. heatmap(corr_fil_food_data. corr(), annot=True, ) It is evident that:  nutrition-score-fr_100g is highly correlated with nutrition-score-uk_100g fat_100g is pretty correlated with saturated-fat_100g nutrition-score-uk_100g is pretty correlated with fat_100g and saturated-fat_100g nutrition-score-fr_100g is pretty correlated with fat_100g and saturated-fat_100g sugars_100g is pretty correlated with carbohydrate_100gLets remove one of these attributes fig, ax = plt. subplots()fig. set_size_inches(15, 15)sns. heatmap(corr_fil_food_data. drop(    ['nutrition-score-fr_100g',     'nutrition-score-uk_100g',     'fat_100g', 'sugars_100g'],    axis=1). corr(), annot=True, ) Result: Using these three simple techniques for dimension reduction, we’ve reduce this dataset from 161 variables down to 49. Do keep in mind that the goal of dimension reduction is to remove attributes that are not very informative. More data does not necessarily mean better and at the same time less data does not necessarily mean better as well. The art is to find a set of attributes within a high dimension data set that will provide sufficient information. new_food_data. shape(134754, 49)"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script>
function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});
</script>

                
            </ul>		
        
        
  
        <!-- End Menu -->

    </div>
        
    </div>
</nav>
<!-- End Navigation
================================================== -->
    
<div class="site-content">     
<div class="container">
    
<!-- Site Title
================================================== -->

<!-- <div class="mainheading">
    <h1 class="sitetitle">The Art of Marketing Science</h1>
    <p class="lead">
         A blog dedicated to Marketing Science covering from concepts to practical examples with code. Oh, it's also my personal data science sandbox for tinkering with data things and go on data science rants.
    </p>
</div> -->

    
    
<!-- Content
================================================== --> 
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
	<div class="row">

		<!-- Post Share -->
		<div class="col-md-2 pl-0">            
           <div class="share">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
        <a target="_blank" href="https://twitter.com/intent/tweet?text=Marvel Cinematic Data Visualization With Plot.ly&url=/marvel-cinematic-plotly/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
        <i class="fab fa-twitter"></i>
        </a>
        </li>
        
        <li class="ml-1 mr-1">
        <a target="_blank" href="https://facebook.com/sharer.php?u=/marvel-cinematic-plotly/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
        <i class="fab fa-facebook-f"></i>
        </a>
        </li>
        
        <li class="ml-1 mr-1">
        <a target="_blank" href="https://plus.google.com/share?url=/marvel-cinematic-plotly/" onclick="window.open(this.href, 'facebook-google', 'width=550,height=435');return false;">
        <i class="fab fa-google"></i>
        </a>
        </li>
        
    </ul>
    
    <div class="sep">
    </div>				
    <ul>
        <li> 
        <a  class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>  
		</div>
		

		<!-- Post -->        
        
        
		<div class="col-md-9 flex-first flex-md-unordered">
			<div class="mainheading">

                <!-- Author Box -->
                				
				<div class="row post-top-meta">
					<div class="col-md-2">
						<!--<img class="author-thumb" src="https://www.gravatar.com/avatar/?s=250&d=mm&r=x" alt="Data Sandbox">-->
                        <img class="author-thumb" src="/assets/images/avatar/green.png?s=250&d=mm&r=x" alt="Data Sandbox">
					</div>
					<div class="col-md-10">
						<a target="_blank" class="link-dark" href="https://artofmarketingscience.github.io/about">Data Sandbox</a><a target="_blank" href="https://twitter.com/fongmanfong" class="btn follow">Follow</a>
						<span class="author-description">Playing is the most natural way to learn. These are the artifacts in my sandbox playground.</span>						
					</div>
				</div>				
                
                
                <!-- Post Title -->
				<h1 class="posttitle">Marvel Cinematic Data Visualization With Plot.ly</h1> 
                
			</div>

			<!-- Post Featured Image -->
			<!-- <img class="featured-image img-fluid" src="/assets/images/2017-06-01-marvel-cinematic-plotly/marvel-cinematic.png" alt="Marvel Cinematic Data Visualization With Plot.ly"> -->
			<!-- End Featured Image -->

			<!-- Post Content -->
			<div class="article-post">
				<p>I’m a huge sucker for Marvel cinematic and in this article I will do a fun exercise with building a simple interactive 3D network graph based on the relationship between Marvel characters. I will be using one of my favourite plotting libraries in Python, Plot.ly. Plot.ly is very easy to use and the way graphs are constructed is very intuitive. The dataset can be found on my GitHub or at the following link: <a href="https://www.kaggle.com/csanhueza/the-marvel-universe-social-network">https://www.kaggle.com/csanhueza/the-marvel-universe-social-network</a></p>

<h3 id="setting-up-and-exploring-the-data">Setting up and exploring the data</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">plotly.plotly</span> <span class="k">as</span> <span class="n">py</span>
<span class="kn">from</span> <span class="nn">plotly.graph_objs</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">plotly.offline</span> <span class="k">as</span> <span class="n">offline</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
</code></pre></div></div>

<p>Key thing to remember with Plot.ly is if you want to build graphs locally on  you computer using Jupyter notebooks, you need to initiate offline notebook mode</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offline</span><span class="o">.</span><span class="n">init_notebook_mode</span><span class="p">()</span>
</code></pre></div></div>

<p>The hero-network.csv dataset contains two columns, hero1 and hero2 and represents a connection between the two characters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">heros</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'hero-network.csv'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">heros</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hero1</th>
      <th>hero2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>574467</td>
      <td>574467</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>6211</td>
      <td>6173</td>
    </tr>
    <tr>
      <th>top</th>
      <td>CAPTAIN AMERICA</td>
      <td>CAPTAIN AMERICA</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>8149</td>
      <td>8350</td>
    </tr>
  </tbody>
</table>
</div>

<p>Some quick data cleaning to remove empty spaces</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">heros</span><span class="p">[</span><span class="n">heros</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">heros</span><span class="p">[</span><span class="n">heros</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">rstrip</span><span class="p">())</span>
</code></pre></div></div>

<p>If you are playing around with the same code and want to explore additional characters, you can use the following line to explore what type of characters are included in the dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#heros[heros['hero1'].str.contains('DAREDEVIL')]['hero1'].unique()
</span></code></pre></div></div>

<p>For this exercise, I’m interested in looking at the connection between some of the marvel characters and villains that we’ve see in theatres!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avengers_name</span> <span class="o">=</span> <span class="p">[</span><span class="s">'HULK/DR. ROBERT BRUC'</span><span class="p">,</span> <span class="s">'BLACK WIDOW/NATASHA'</span><span class="p">,</span> <span class="s">'CAPTAIN AMERICA'</span><span class="p">,</span> <span class="s">'IRON MAN/TONY STARK'</span><span class="p">,</span> 
                 <span class="s">'WAR MACHINE II/PARNE'</span><span class="p">,</span> <span class="s">'HAWKEYE | MUTANT X-V'</span><span class="p">,</span> <span class="s">'FALCON/SAM WILSON'</span><span class="p">,</span> 
                 <span class="s">'SCARLET WITCH/WANDA'</span><span class="p">,</span> <span class="s">'VISION'</span><span class="p">,</span> <span class="s">'ANT-MAN II/SCOTT HAR'</span><span class="p">,</span> <span class="s">'SPIDER-MAN/PETER PAR'</span><span class="p">,</span> 
                 <span class="s">"BLACK PANTHER/T'CHAL"</span><span class="p">,</span> <span class="s">'DR. STRANGE/STEPHEN'</span><span class="p">,</span> <span class="s">'THOR IV/DARGO'</span><span class="p">,</span> <span class="s">'FURY, COL. NICHOLAS'</span><span class="p">,</span> 
                 <span class="s">'QUICKSILVER/PIETRO M'</span>
                <span class="p">]</span>

<span class="n">villain_name</span> <span class="o">=</span> <span class="p">[</span> <span class="s">'BUCKY/BUCKY BARNES'</span><span class="p">,</span> <span class="s">'MALEKITH/MALCOLM KEI'</span><span class="p">,</span> <span class="s">'THANOS'</span><span class="p">,</span> <span class="s">'ULTRON'</span><span class="p">,</span>
                 <span class="s">'LOKI [ASGARDIAN]'</span><span class="p">,</span> <span class="s">'BARON MORDO/KARL MOR'</span><span class="p">,</span> <span class="s">'DORMAMMU'</span><span class="p">,</span> <span class="s">'RED SKULL/JOHANN SCH'</span><span class="p">]</span>

<span class="nb">all</span> <span class="o">=</span> <span class="p">[]</span>
<span class="nb">all</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">avengers_name</span><span class="p">)</span>
<span class="nb">all</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">villain_name</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="first-lets-just-explore-the-avengers-social-circle">First lets just explore The Avenger’s social circle</h3>

<p>We do a quick count on how many relationships each avengers have and how many comic books they appeared in</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avenger_info</span> <span class="o">=</span> <span class="p">{</span><span class="s">'hero'</span><span class="p">:</span> <span class="p">[],</span> <span class="s">'buddies'</span><span class="p">:</span> <span class="p">[]}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">avengers_name</span><span class="p">:</span>
    <span class="n">avenger_info</span><span class="p">[</span><span class="s">'hero'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">avenger_info</span><span class="p">[</span><span class="s">'buddies'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">heros</span><span class="p">[</span><span class="n">heros</span><span class="p">[</span><span class="s">'hero1'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avenger_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">avenger_info</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'hero'</span><span class="p">,</span> <span class="s">'buddies'</span><span class="p">,</span> <span class="s">'comics'</span><span class="p">])</span>
</code></pre></div></div>

<p>Building a grouped bar chart in Plotly is very simple. Each set of bar is treated as seperate data, we define the x and y values and the aesthetics for each group of bars. Then we define the layout like axes, chart title etc. Lastly we pass data and layout into Plot.ly’s figure function to build the graph</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">buddies</span> <span class="o">=</span> <span class="n">Bar</span><span class="p">(</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">avenger_df</span><span class="p">[</span><span class="s">'hero'</span><span class="p">],</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">avenger_df</span><span class="p">[</span><span class="s">'buddies'</span><span class="p">],</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'buddies'</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'rgba(234, 35, 40, 0.7)'</span><span class="p">,</span>
        <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">color</span><span class="o">=</span><span class="s">'rgba(234, 35, 40, 1.0)'</span><span class="p">,</span>
            <span class="n">width</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">buddies</span><span class="p">]</span>
<span class="n">layout</span> <span class="o">=</span> <span class="n">Layout</span><span class="p">(</span>
    <span class="n">barmode</span> <span class="o">=</span> <span class="s">'group'</span><span class="p">,</span>
    <span class="n">bargroupgap</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">title</span> <span class="o">=</span>  <span class="s">'Avengers - Buddies'</span>
<span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">Figure</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">layout</span> <span class="o">=</span> <span class="n">layout</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</code></pre></div></div>

<iframe width="900" height="500" frameborder="0" scrolling="no" src="//plot.ly/~datafong/8.embed"></iframe>

<p>We can see our fellow Captain Steve Rogers is quite popular along with friendly neighborhood Spiderman and Mr. Tony Stark. One weird data point here is Thor who in my mind is should be quite popular… This might be because there are several Thor characters in the dataset, representing different Thors from different universes, as well as the comic book universe being different from cinematics.</p>

<h3 id="now-lets-build-the-network-graph-for-our-avengers-and-villains">Now lets build the network graph for our Avengers and villains</h3>

<p>First off, there are a whole lot of characters in our dataset, lets just keep our avengers and villains</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">keep_avengers</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">characters</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="s">'hero1'</span><span class="p">]</span> <span class="ow">in</span> <span class="n">characters</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'hero2'</span><span class="p">]</span> <span class="ow">in</span> <span class="n">characters</span><span class="p">)):</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">heros</span><span class="p">[</span><span class="s">'keep'</span><span class="p">]</span> <span class="o">=</span> <span class="n">heros</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">keep_avengers</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">all</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">heros</span> <span class="o">=</span> <span class="n">heros</span><span class="p">[</span><span class="n">heros</span><span class="p">[</span><span class="s">'keep'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'keep'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'index'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Next we use the igraph libary which is a library for high-performance graph generation and analysis. For more information on installation, visit <a href="http://igraph.org/python/">http://igraph.org/python/</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">igraph</span> <span class="k">as</span> <span class="n">ig</span>
</code></pre></div></div>

<p>One of the requirement to build this network graph is to express the source and destination nodes as integer values. So we start off with encoding our heros into numbers</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">heros</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hero1</th>
      <th>hero2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SPIDER-MAN/PETER PAR</td>
      <td>HULK/DR. ROBERT BRUC</td>
    </tr>
    <tr>
      <th>1</th>
      <td>QUICKSILVER/PIETRO M</td>
      <td>SCARLET WITCH/WANDA</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QUICKSILVER/PIETRO M</td>
      <td>IRON MAN/TONY STARK</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Edges</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mapper</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">character_group</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">unique_heros</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">heros</span><span class="p">[</span><span class="s">'hero1'</span><span class="p">],</span> <span class="n">heros</span><span class="p">[</span><span class="s">'hero2'</span><span class="p">]])</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="n">num_unique_heros</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_heros</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unique_heros</span><span class="p">):</span>
    <span class="n">mapper</span><span class="p">[</span><span class="n">unique_heros</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">i</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">heros</span><span class="p">[</span><span class="s">'hero1_node'</span><span class="p">]</span> <span class="o">=</span> <span class="n">heros</span><span class="p">[</span><span class="s">'hero1'</span><span class="p">]</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mapper</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
<span class="n">heros</span><span class="p">[</span><span class="s">'hero2_node'</span><span class="p">]</span> <span class="o">=</span> <span class="n">heros</span><span class="p">[</span><span class="s">'hero2'</span><span class="p">]</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mapper</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">heros</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hero1</th>
      <th>hero2</th>
      <th>hero1_node</th>
      <th>hero2_node</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SPIDER-MAN/PETER PAR</td>
      <td>HULK/DR. ROBERT BRUC</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>QUICKSILVER/PIETRO M</td>
      <td>SCARLET WITCH/WANDA</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>QUICKSILVER/PIETRO M</td>
      <td>IRON MAN/TONY STARK</td>
      <td>1</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

<p>Next I’d like to seperate our good guys from the bad guys visually, so let’s group them up.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">unique_heros</span><span class="p">:</span>
    <span class="n">character_group</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">j</span> <span class="ow">in</span> <span class="n">avengers_name</span><span class="p">)</span> <span class="k">else</span> <span class="n">character_group</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Now were ready to build our nodes and links. We start off with links by putting our (source, destination) for every node into Edges and pass Edges into our igraph. The layout function here defines the overall structure of our network graph and we use ‘sphere’. There are other options like</p>

<ul>
  <li>
    <p>gfr, grid_fr, grid_fruchterman_reingold: grid-based Fruchterman-Reingold layout</p>
  </li>
  <li>
    <p>kk, kamada_kawai: Kamada-Kawai layout</p>
  </li>
  <li>
    <p>kk_3d, kk3d, kamada_kawai_3d: 3D Kamada-Kawai layout</p>
  </li>
</ul>

<p>Play around with some of these to see the different structures.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">heros</span><span class="p">)):</span>
    <span class="n">Edges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">heros</span><span class="p">[</span><span class="s">'hero1_node'</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">heros</span><span class="p">[</span><span class="s">'hero2_node'</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">G</span> <span class="o">=</span> <span class="n">ig</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="n">Edges</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">layt</span><span class="o">=</span><span class="n">G</span><span class="o">.</span><span class="n">layout</span><span class="p">(</span><span class="s">'sphere'</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<p>This part looks a little intimidating and complicated but its not so bad. Layout function helps us define the layout of the network graph, what we are doing here is populating our X, Y, Z coordinates for each node and edge to be placed into our 3D space</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xn</span><span class="o">=</span><span class="p">[</span><span class="n">layt</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unique_heros</span><span class="p">)]</span><span class="c1"># x-coordinates of nodes
</span><span class="n">Yn</span><span class="o">=</span><span class="p">[</span><span class="n">layt</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unique_heros</span><span class="p">)]</span><span class="c1"># y-coordinates
</span><span class="n">Zn</span><span class="o">=</span><span class="p">[</span><span class="n">layt</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unique_heros</span><span class="p">)]</span><span class="c1"># z-coordinates
</span><span class="n">Xe</span><span class="o">=</span><span class="p">[]</span>
<span class="n">Ye</span><span class="o">=</span><span class="p">[]</span>
<span class="n">Ze</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">Edges</span><span class="p">:</span>
    <span class="n">Xe</span><span class="o">+=</span><span class="p">[</span><span class="n">layt</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="mi">0</span><span class="p">],</span><span class="n">layt</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="mi">0</span><span class="p">],</span> <span class="bp">None</span><span class="p">]</span><span class="c1"># x-coordinates of edge ends
</span>    <span class="n">Ye</span><span class="o">+=</span><span class="p">[</span><span class="n">layt</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span><span class="n">layt</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="mi">1</span><span class="p">],</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">Ze</span><span class="o">+=</span><span class="p">[</span><span class="n">layt</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="mi">2</span><span class="p">],</span><span class="n">layt</span><span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]][</span><span class="mi">2</span><span class="p">],</span> <span class="bp">None</span><span class="p">]</span>
</code></pre></div></div>

<p>Same as how we built the bar chart, we define our data &amp; layout and pass into the Figure function. Instead of bar chart, we are using Scatter3d for our data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace1</span><span class="o">=</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">Xe</span><span class="p">,</span>
               <span class="n">y</span><span class="o">=</span><span class="n">Ye</span><span class="p">,</span>
               <span class="n">z</span><span class="o">=</span><span class="n">Ze</span><span class="p">,</span>
               <span class="n">mode</span><span class="o">=</span><span class="s">'lines'</span><span class="p">,</span>
               <span class="n">line</span><span class="o">=</span><span class="n">Line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">'rgb(125,125,125)'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
               <span class="n">hoverinfo</span><span class="o">=</span><span class="s">'none'</span>         
            <span class="p">)</span>

<span class="n">trace2</span><span class="o">=</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">Xn</span><span class="p">,</span>
               <span class="n">y</span><span class="o">=</span><span class="n">Yn</span><span class="p">,</span>
               <span class="n">z</span><span class="o">=</span><span class="n">Zn</span><span class="p">,</span>
               <span class="n">mode</span><span class="o">=</span><span class="s">'markers'</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s">'actors'</span><span class="p">,</span>
               <span class="n">marker</span><span class="o">=</span><span class="n">Marker</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s">'dot'</span><span class="p">,</span>
                             <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>
                             <span class="n">color</span><span class="o">=</span><span class="n">character_group</span><span class="p">,</span>
                             <span class="n">colorscale</span><span class="o">=</span><span class="p">[</span>
                                <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s">'black'</span><span class="p">],</span>
                                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="s">'red'</span><span class="p">]],</span>
                             <span class="n">line</span><span class="o">=</span><span class="n">Line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">'rgb(50,50,50)'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
                             <span class="n">opacity</span> <span class="o">=</span> <span class="mf">0.8</span>
                             <span class="p">),</span>
               <span class="n">text</span><span class="o">=</span><span class="n">unique_heros</span><span class="p">,</span>
               <span class="n">hoverinfo</span><span class="o">=</span><span class="s">'text'</span>  
            <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">axis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showbackground</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">showline</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">zeroline</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">showgrid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">showticklabels</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">title</span><span class="o">=</span><span class="s">''</span><span class="p">,</span>
          <span class="n">showspikes</span> <span class="o">=</span> <span class="bp">False</span>
          <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layout</span> <span class="o">=</span> <span class="n">Layout</span><span class="p">(</span>
         <span class="n">title</span><span class="o">=</span><span class="s">"Marvel Cinematic - Social Network"</span><span class="p">,</span>
         <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
         <span class="n">height</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span>
         <span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
         <span class="n">scene</span><span class="o">=</span><span class="n">Scene</span><span class="p">(</span>
         <span class="n">xaxis</span><span class="o">=</span><span class="n">XAxis</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span>
         <span class="n">yaxis</span><span class="o">=</span><span class="n">YAxis</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span>
         <span class="n">zaxis</span><span class="o">=</span><span class="n">ZAxis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">hovermode</span><span class="o">=</span><span class="s">'closest'</span>   
<span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="o">=</span><span class="n">Data</span><span class="p">([</span><span class="n">trace1</span><span class="p">,</span> <span class="n">trace2</span><span class="p">])</span>
<span class="n">fig</span><span class="o">=</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</code></pre></div></div>

<iframe width="900" height="800" frameborder="0" scrolling="no" src="//plot.ly/~datafong/6.embed"></iframe>

			</div>

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2017-06-01">01 Jun 2017</time></span>           
                
                </small>
            </p>
            
			<!-- Post Categories -->
			<div class="after-post-tags">
				<ul class="tags">
                    
                    
                    <li>                        
                     <a class="smoothscroll" href="/categories.html#data sandbox">Data Sandbox</a>
                    </li>
                    
				</ul>
			</div>
			<!-- End Categories -->
            
            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="/dimension-reduction-pca/"> &laquo; Dimension Reduction with Principal Component Analysis</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="/vanderbilt-university-regression-modelling-strategies-course/">Vanderbilt Regression Modelling Strategies Course &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

		</div>
		<!-- End Post -->

	</div>
</div>
<!-- End Article
================================================== -->

  

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">              
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'https-fongmanfong-github-io'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
                
            </div>
        </div>
    </div>

<!--End Comments
================================================== -->
</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt="The Art of Marketing Science"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://github.us20.list-manage.com/subscribe/post?u=7e19d52d70f2181f9f5f666db&amp;id=2158c5e28a" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>    

    
</div><!-- /.container> -->
    
<!-- Categories Jumbotron
================================================== -->
<!--<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">      
            
            
            
            <a href="/categories#data-sandbox">Data Sandbox (4)</a>
            
            <a href="/categories#art-of-marketing-science">Art of Marketing Science (5)</a>
            
            
            
		</div>
	</div>
</div>
-->
 


<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                 Copyright © 2020 The Art of Marketing Science 
            </div>
            <!--<div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>-->
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

   
</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
    
<script src="/assets/js/mediumish.js"></script>
<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 
    
<script id="dsq-count-scr" src="//https-fongmanfong-github-io.disqus.com/count.js"></script>
    
</body>
</html>
