<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">
    
<!-- <title>404 | The Art of Marketing Science</title>-->

        
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>404 | The Art of Marketing Science</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="404" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data science in marketing with practical examples" />
<meta property="og:description" content="Data science in marketing with practical examples" />
<meta property="og:site_name" content="The Art of Marketing Science" />
<script type="application/ld+json">
{"description":"Data science in marketing with practical examples","@type":"WebPage","url":"/404.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"headline":"404","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
    
<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
    
<link href="/assets/css/screen.css" rel="stylesheet">
    
<link href="/assets/css/main.css" rel="stylesheet">
    
<script src="/assets/js/jquery.min.js"></script>
    
</head>
    

    

<body class="layout-default">

    
<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">
    
    <div class="container pr-0">    
    
    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="The Art of Marketing Science">
    </a>
    <!-- End Logo -->
  
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    
    <div class="collapse navbar-collapse" id="navbarMediumish">
       
        <!-- Begin Menu -->
        
            <ul class="navbar-nav ml-auto">
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Home</a>
                </li>
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/about">About</a>
                </li>
                
                <li class="nav-item">
                <a class="nav-link" href="https://github.com/fongmanfong/Resources">Resources</a>
                </li>
            
                
                <script src="/assets/js/lunr.js"></script>

<script>
    

var documents = [{
    "id": 0,
    "url": "/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "/about",
    "title": "About",
    "body": "	The Art of Marketing Science is a blog dedicated to the topic of applied data science in marketing. Here we explore different ways of looking at marketing problems through the lense of data science as well as demonstrate examples with code included.  	In addition, this blog includes a `Data Sandbox` section that serves as a personal data science playground for me. You'll find a variety of topics in this topic including tutorials, examples, random data thoughts and rants.   The AuthorMy name is Jason Fong but more popularly referred to as fongmanfong. I currently lead the marketing data science team at Shopify. Learn more about me "
    }, {
    "id": 2,
    "url": "/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "/",
    "title": "Art of Marketing Science",
    "body": "{% if page. url == “/” %}       Featured:           {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}      {% endif %}           All Stories:               {% for post in paginator. posts %}        {% include postbox. html %}    {% endfor %}          {% include pagination. html %}"
    }, {
    "id": 4,
    "url": "/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "/problems_before_tools_an-alternative_view_to_generalist_data_scientist/",
    "title": "Problems before tools: An alternative perspective to the full stack data science generalist",
    "body": "2019/03/29 -  I recently read an article from Eric Colson, Chief Algorithm Officer at Stitch Fix, where he talked about how we should avoid building data science teams like a manufacturing plant, comprising of highly specialized individuals operating different parts of a manufacturing process. Instead data science teams should be built with a full stack approach where data scientists are considered to be generalists. Generalists refers to the capability of performing diverse functions from conception to modelling to implementation to measurement. I won’t go into a detail summary of the article here but you should read Eric’s article before continuing on. The purpose of this article is to provide a complementary view into Eric’s philosophy. In his article, he took a very top down approach to describing why a data science team should be built with generalists. I believe the same conclusion can be drawn through the lens of a bottoms up approach, from a perspective of a practitioner of data science and what it really means to be in data science. Let’s start this discussion of with just defining data science. What exactly is data science or what does a data scientist do? Looking to our friendly neighbourhood Mr. Interweb for some help, here are some definitions that I’ve found. Tell Me Sir, What Is This Data Science You Refer To?: “Data science is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. ” – Wikipedia “Data science is a “concept to unify statistics, data analysis, machine learning and their related methods” in order to “understand and analyze actual phenomena” with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science. ” – Wikipedia Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract value from data. Data scientists combine a range of skills—including statistics, computer science, and business knowledge—to analyze data collected from the web, smartphones, customers, sensors, and other sources. - Oracle “Data science is the field of study that combines domain expertise, programming skills, and knowledge of math and statistics to extract meaningful insights from data. Data science practitioners apply machine learning algorithms to numbers, text, images, video, audio, and more to produce artificial intelligence (AI) systems that perform tasks which ordinarily require human intelligence. In turn, these systems generate insights that analysts and business users translate into tangible business value. ” – Data Robot These are some nicely crafted definitions (way better than I would have been able to articulate it) but an observable and consistent pattern across these definitions (along with numerous other on the internet) is that data science tends to be defined by doing a set of specific tasks or using a set of specific tools. If you do X, Y, Z and use A, B, C then that’s considered data science. The causal chain would look something this: Data Science - The Most Unsexy Field In The 21st Century Practicing data science will lead you to use math and statistics, apply machine learning algorithm, learn programming, increase domain expertise. Why? First, I think through the process of packaging up and marketing data science to be this sexy field, we have incorrectly defined the definition of data science. I don’t think the definitions mentioned above are incorrect but I think a definition like this one is more authentic: “Data science, in its most basic terms, can be defined as obtaining insights and information, really anything of value, out of data. Like any new field, it’s often tempting but counterproductive to try to put concrete bounds on its definition. ” – Thinkful To make the definition even less sexy, I believe data science should be simply: “Just using data to solve problems”. Data scientist should be “the data girl or the data guy”. Not so sexy anymore. Though we still have all the same elements in the two causal chains, the subtle difference in reversal of causality actually leads to a big difference in how data science is practiced in reality. This is the same concept as viewing a cup of water as half full vs. half empty. Although both views are correct, there is a significant difference in how individuals that see things as half full behave and act versus individuals that sees it as half empty. In the world through the first causal chain, we see a lot of data science practitioners that are solely interested in specific tasks of data science. Have you met anybody that explicitly indicated they are only interested in building models and don’t care about analytics or working with stakeholders? Odds are you probably have. In the world through the second causal chain, specific data science tasks are performed because it helps solve the problem at hand. How Does This Relate to Being A Generalist?: So, after all of this detour, how does any of this stuff relate to being a generalist? I believe if we looked at data science the right way, the only way to really do data science optimally is to be a generalist (once again, love to hear your thoughts and opinions in the comments below). Why? If we always start with the problem being the main focus, we will realize that if we wanted to solve the problem the best way possible, we cannot simply just do one part of data science and not the other part. Echoing Eric in his article, a generalist role provide all the things that drive job satisfaction: autonomy, mastery and purpose. Especially around mastery, if we always start with the problem, we can determine if our current toolkit can adequately answer the question or we need to branch deeper and apply new data science techniques or technology to help address the problem. "
    }, {
    "id": 6,
    "url": "/AOM-geo-experiments-part1/",
    "title": "Geo Experiments Part 1: What Is It and How Will It Help You In Marketing?",
    "body": "2019/03/25 - This will be a three-part series discussing the topic of Geo Experimentation and its use in marketing. Part 1: What Is It and How Will It Help You In Marketing? Part 2: Understanding the mathematics behind Geo Experiments Part 3: Application of Geo Experiments with examples and R code The Bread &amp; Butter of Test &amp; Learn: A/B testing (aka split testing) has been essential in helping marketers remove the guesswork and make data-informed decisions. For most marketers, this comes in the form of landing page optimization. You split traffic into different groups, expose some changes to the page to one of the groups while keeping everything else the same for the other, measure the difference for a specific metric of interest (signup rate, CTR etc) and determine if the difference between the two groups is statistically significant. Some paid marketing specialists may have worked with A/B testing in Google Ads or Facebook Ads, where the audience is subjected to different ad formats or ad copy. Similar methodology for both but applied differently. So why do marketers run A/B tests? This is where marketers put on their scientist hat. Marketers run A/B testing to understand the causal and incremental impact of something, whether it be a change on a landing page or ad copy for their Facebook campaign. They want to know, for this specific thing that they did, what is the incremental impact that it had, so that they can make informed decisions about what they should do next. A/B testing has been the bread and butter for marketers to do tests and learn. But if I were to ask you to design an A/B test to answer each of the following questions, how would you go about doing so?  Understand the incremental impact of a Facebook campaign on customer conversion? (Meaning, I want to know the actual impact on overall customer conversion and not what’s reported in your Facebook Business Manager under the conversion column) Understand the effect of billboard advertisement on visits to your websiteGive it some thought.  Let’s go through each question and see why our favourite A/B test doesn’t work:    You can A/B test your Facebook campaign to understand the incremental impact for metrics like CTR or engagement rate but customer conversion is a little different. Why? Because under your whole measurement framework lies an attribution model that assigns credit in a non-perfect way. As a result, even though you are running an A/B test to understand customer conversion, it only provides you with a relative comparison between the two campaigns. It does not provide an overall view of whether the campaign brought in incremental customers. You do not know if whether you’re simply taking customer conversions away from other channels.     This one is straightforward. Firstly, you just don’t have any direct response metrics to measure. Secondly, how do you plan to create your control and test group? Do you plan to blindfold half the population in a city that passes by the billboard to ensure they don’t see the billboard while the letting other half see it?  A/B testing does not work when you do not have control over your population or have a direct measure of the metric of interest. Although A/B testing is the bread and butter for marketers we also see it’s limitations and the need for additional tooling to help marketers answer important questions. The Bread &amp; Olive Oil: Geo Experiments: Sometimes bread and butter just aren’t enough and you got to switch things up. Bread and olive oil usually goes well together too. So what exactly is Geo Experiment? Well, in simplest terms, a Geo Experiment is like an A/B test but uses geographies to define your control and treatment group rather than individuals or web cookies. In Geo Experiments, geographic regions are divided into treatment and control group. The regions in the treatment group are exposed to a marketing intervention while regions in the control group remain status quo. The marketing intervention happens for a duration of time and the response metric is observed. The idea is to detect if there is an incremental lift in the response metric in the treatment regions. Let’s use the Facebook question above as an example and assume we are running the ad campaign in the United States. First, we randomize a set of cities in the United States putting them into treatment and control group. Just like A/B test, randomization is a critical part of the experimental design. Then in Facebook Business Manager, we set the campaign to only run in the cities in the treatment group. We run the campaign for a duration of time and we measure the incremental customers acquired (agnostic of an attribution model) by comparing the difference between the control vs. treatment group. For example, we may see something like: This is a graph produced by the GeoexperimentsResearch R package developed at Google which implements the Geo Experiment methodology and provides an easy way to perform the analysis. I’ll be demonstrating how to use this R package in Part 3! But at a high level, the y-axis can be customer conversion for our example purposes, the x-axis is divided into three distinct periods:  Pre-test period (Feb 05 — Mar 31): before the campaign started Test period (Apr 01 — Apr 28): when the campaign is running Cooldown period (Apr 29 — May 05): campaign stops running but there might be a lingering effect of the ad that trickle inThe top graph illustrates the observed response metric overtime vs the predicted counterfactual. We’ll talk more about this in detail in Part 2 but essentially the counterfactual is what we would have observed if the marketing campaign did not run. The middle graph illustrates the difference between the observed and counterfactual by day which estimates the lift by day. Finally, the bottom graph is the total lift we’ve observed in the experiment. My explanation above is an oversimplification of the methodology. To truly understand what’s going on, you need to go inside the hood and look at the engine. The methodology and math behind Geo Experiment are different from a traditional A/B test but the overall idea is similar. As mentioned, I will be leaving the detailed explanation of the mathematics to Part 2 of this series. The methodology that I’ll be going through in Part 2 is research from Google, so if you are interested, take a look first. A one-liner explanation: a regression model is used to learn the exchangeability factor between the control regions and the treatment regions and then used to predict the counterfactual during the intervention period. Maybe this sentence doesn’t make any sense to you at all. Don’t worry, I will be breaking this down in simple terms! Stay tuned! I want to take a short moment to highlight that Geo Experiment is another use case of regression in marketing science. If you haven’t read my article “One Thing Marketing Analysts Should Have In Their Analytics Toolkit”, take a look to see why regression is a tool every Marketing Analyst should have in their toolkit. By no means is Geo Experiment designed to replace traditional A/B test. They serve very different purposes and answers different questions but both are equally important. Just as they say, “there is no one size fits all”, you shouldn’t just have butter with your bread. Go ahead and add some variety, olive oil is great too! Challenges and Limitations: By now, I might have sold you on how great this new tool is BUT, like any other tools, Geo Experiment comes with its own set of limitations and challenges:  Overhead Cost: There may be a lot more work involved to set up a Geo Experiments depending on how you’ve set up your campaigns. You may need to restructure your existing digital marketing accounts in a way that allows you to target campaigns at a city/region level to create your control and treatment group.  Platform: Not every marketing platform you advertise on allows you to target campaigns at a city/region level. For example, you may be able to target Facebook paid ads at a city/region level but you don’t have that capability when you are running ads in podcasts.  Budget: Depending on your business, campaign, and many other factors, the budget you need for a campaign will greatly vary. For companies that are just starting out with digital marketing, I wouldn’t be looking at Geo Experiments just yet. If your business is mature and you’re looking to optimize, then I think its a good fit.  Controlling for variables: Can you control for marketing activities happening across different cities? E. g are there other marketing initiatives from your organization happening in specific cities that might overlap with your Geo Experiment?And finally, alway’s remember to choose the right tool for the right job. There is no one size fits all! "
    }, {
    "id": 7,
    "url": "/AOM-one_thing_marketing_analysts_should_have_in_their_analytics_toolkit/",
    "title": "One Thing Marketing Analysts Should Have In Their Analytics Toolkit",
    "body": "2018/12/31 - Let’s be real here, being a data scientist ain’t easy. Have you seen what a modern data scientist should know?! 😱 That’s a pretty extensive list of requirements but how feasible and practical is it to know everything on this list? In my opinion, not that feasible nor practical. In reality, a data scientist shouldn’t need to know everything mentioned (at least with the same level of competency). Instead, the problem space that a data scientist is working in should define the set of tools he/she needs. Similar to doctors, you either have family doctors who are generalists or you have doctors specializing in different areas. You don’t expect one doctor to know everything and be a specialist in everything. So for a marketing analyst or data scientist in marketing, what are some tools they should have in their toolkit? Most of the time we hear about things like machine learning, R, Python, domain knowledge, communication skills etc. but one tool that has not gotten enough spotlight in marketing is regression. To answer why regression should be in every marketing analysts analytics toolkit, we should first think about what makes a good tool for marketing. What Makes A Good Tool For Marketing?: And to answer this question, we should think about what problems or questions a marketing organization or a CMO needs to answer. Some bigger questions include:  What is the growth and revenue for the company? What is a customer’s lifetime value? Who is at risk of churning? Who should we send this promotional offer to? What is the ROI of my advertising investment?In theory, a good tool should help with answering some of these questions (but it would be naive to think that there is one tool that can solve all the problems aka No Free Lunch Theorem). In addition to just answering these questions, what’s equally as important for a CMO is understanding what levers are moving the needle.  For example, it’s not enough to know whether a company is growing or if revenue is increasing, a CMO needs to know what is moving the needle. Who is at risk of churning and what signals indicate a high risk of churning? Therefore a good tool in marketing should not only help with answering some of these questions but also provide insight into what variables are moving the needle. What is Regression?: I won’t be going into any details on regression in this article. I have some resources at the end if you are interested in learning more about regression. At a high-level, regression is a family of statistical analysis techniques that examine the relationships between variables. There are different types of regressions (logistic, linear, survival etc) in the family but the overall objective is to model the relationship between variables. For example, we may be interested in understanding how investment in each marketing channel influences customer acquisition. We can build a model where the independent variables (factors you hypothesize to have a relationship) is the investment in each marketing channel and the dependent variable (factors you are trying to understand) being customers acquired. It could hypothetically look something like Customers Acquired = B0 + B1 * AdWords_Spend + B2 * Facebook_Spend + B3 * Snapchat_Spend Oh No, Equations and Formulas 😰: The beauty of regression lies in the equation. Why? Because if we’ve modeled the data appropriately, we can make interpretations of the coefficients (the Betas) in the equation to make decisions. This is different than some of the more popular machine learning techniques like deep learning, random forest etc. which are powerful tools for making predictions but the blackbox nature of the method makes it not interpretable. Depending on how the data is modeled (e. g. logistic vs linear vs log-log) we can interpret the coefficient in different ways to understand the relationship between a dependent variable to the independent variable. Let’s take the equation above as an example: we’ve used linear regression to model marketing spend versus customers acquired. The coefficients can be interpreted as a dollar spent in Google Adwords gets 2 number of customers. Customers Acquire = 100+ 2. 0 * AdWords_Spend + 1. 3 * Facebook_Spend + 0. 5 * Snapchat_Spend In other words, the coefficients can be interpreted as the levers that a CMO would be interested in knowing. Regression also provides nice statistical properties including, but not limited to, statistical significance, confidence interval, &amp; standard error of each estimator (our coefficients of interest) which provides a CMO with upper/lower bound of estimates and level of confidence for our estimates. (In From Marketing Analytics To Marketing Science I talk more about why we should look at marketing problems from more of a scientific lens) Swiss Army Knife of Data Science: We should also appreciate regression for its versatility and the wide variety of applications within marketing. Regression is primarily used for:  Predictive Analytics — regression is also a foundational piece for predictive analytics. This is what we’ve been talking about so far. We model the relationships between variables to make predictions about the value of the independent variable based on what we know for the dependent variables.  Causal Inference — extending beyond just modeling the relationship between variables, we can leverage regression to make causal inference. Meaning making causal claims between variables. Some applications in marketing include:  Geo experimentation Causal impact of marketing events including IRL / Offline Marketing Leading scoring Product marketing and upsell / cross-sell Media mix modeling and budget optimization ForecastingLearning Regression: I wish learning regression was as simple as doodling some dots and drawing a line through them but the reality is that it is a vastly deep and complicated subject. But there are good news!  There is a saying “Nothing in life worth having come easy. ” Therefore regression must be worth having.  You don’t need to master regression before applying it. It is one of those subjects that you build up knowledge over time. There are plenty of Ph. D. students in statistics still learning regression. I have a couple links below as a starting point. If you have any good resources, please share in the comments below as well for others! Introduction To Regression Model (Coursera) Penn State — Stats 501 Regression Methods Mastering ‘Metrics: The Path from Cause to Effect Regression Modeling Strategies — Frank Harrell (Textbook) Regression Modeling Strategies — Frank Harrell (Course) "
    }, {
    "id": 8,
    "url": "/AOM-from_marketing_analytics_to_marketing_science/",
    "title": "From Marketing Analytics To Marketing Science",
    "body": "2018/12/10 - “Half the money I spend on advertising is wasted; the trouble is I don’t know which half. ” The changes in the technology landscape in the past two decades have evolved the marketing industry as a whole, creating an entirely new field called digital marketing. First the dot com and internet boom, followed by an explosion of web data, and finally creation of cheap and readily available web analytics tools, digital marketers have much more insight into how their marketing campaigns are performing. Still which this much data and tools, marketers still have trouble answer this question: Web analytic tools like Google Analytics &amp; Adobe Omniture has made marketing analytics a commodity and a marketer’s best friend. With a click of few button, marketers have the ability to see how many clicks, impression and even conversion their marketing campaigns have generated. They can see how their campaigns stack up against each other and overtime, turning marketing into a analytics and metrics driven profession.  But since then, the move from purely analytics approach to more science-based has been slow. You probably now questioning what I mean by science vs analytics here so let’s take a quick moment to explain how I define the two: Analytics approach → Answering business questions by leveraging descriptive and predictive analytics  Looking at the data overtime and identifying trends. Are things doing better or worse? Segmenting / slicing &amp; dicing the data to look at it from different perspectives. Are certain segments doing better or worse? Making decisions based on correlation between variables. When X moves, Y seems to move as well, so let’s do more of X.  Using the data to make predictions in the future. Will this person churn?Science approach → interpreting data and drawing inference with scientific rigor  Attempting to find causality between variables, what’s causing what and at what magnitude Looking at data in terms of estimate, probability, confidence interval, posterior distribution etc. I’m not sure what X is but I’m fairly it’s between Y and Z Experimental design to prove/disprove hypothesis. Running randomized trial. One science approach that marketing has adopted is experimental design in the form of A/B testing. Although widely used within marketing (and other areas), I’d argue a lot of the designed experiments implemented in many corporations are questionable. This will be a separate post. I believe that marketing analytics technology has matured enough and as marketers and data analyst, we should look at marketing from a scientific lens in additional to traditional analytics. I’m Not Sure If I Understand The Difference: My explanation above might be too theoretical and high level. Let’s go through an example to better understand. Assume Sally is a marketer and she does SEM for Company X. She sets up a new campaign targeting Company X as the keyword. After a week she sees that her campaign received 100K impressions, 5000 clicks, 50 customers and spent a total of $500. The first question that needs to be answered is: “are the results good or bad”? How would Sally go about answering this question? A general approach would be benchmark her new campaign with existing campaigns, comparing CPC and CAC. Let say she compares across several campaigns and on average CPC is around $0. 20 and CAC is around $20, meaning her new campaign is 2x more efficient. This is great, seems like this campaign is killing it, let’s double down and spend more on this campaign. This is typically how marketing data is interpreted and actioned on. Marketers and data analyst looks at overall trends and make a decision based on insight drawn. So what’s the problem here? Well several key problems:  We never actually answered whether the campaign is performing well. We only looked at whether this campaign performed relatively well compared to historical aggregate of data What if all this campaign did was take from other campaigns? What is the probability that the results we saw happened by chance?What we should be interested in here instead is the incremental benefit of having this campaign to get a true sense of CAC. Sure the results might look great at face value but what if all it’s doing is taking away traffic from your Organic Search. Meaning these clicks would of been captured through Organic Search anyways. Actually in this case, the campaign is adding negative value since now Company X is paying for “free” traffic it would of gotten anyways. Ok, So How Can I Be More Scientific With Marketing?: This is just one example (though very common) of using marketing analytics to drive “insights”. As seen, the problem with pure analytics approach is that we might not be answering the true question of interest. At best we may be making decisions based on signals that are highly correlated with the truth, at worst we are making decisions that are purely noise. Imagine if your company has a nine figure marketing budget, every decision becomes very expensive and critical to get right.  So how can we answer this question? Methods and techniques to answer this question goes back to what this article is about, scientific approaches including experimental design and regression analysis can be used to help answer this question. I won’t go into too much detail in this introductory article (will in future articles) but designing a geo experiment or building a regression based model to understand the counterfactual are techniques that can be used. A Mindset Shift: Evolving from marketing analytics to marketing science first requires a shift in mindset. We need to understand that more data does not mean more data informed decisions. We need to look at each problem and insight and critic it with rigor to determine if we are simply making inference on correlation or causation. I’m not saying that analytics is not useful to make decisions, far from that. Analytics provide a way to draw quick “insights” (better than just throwing darts in the dark). What I’m highlighting is the need to complement analytics with more science to make sure we’re hitting bullseye. "
    }, {
    "id": 9,
    "url": "/DS-regression_modelling_strategies/",
    "title": "Regression Modelling Strategies",
    "body": "2018/06/02 - I feel very fortunate to work for an employer like Shopify, that places a lot of value in continuous learning and self development. One of the many benefits that we get as employees is an annual self-development budget that we can use on anything to level-up ourselves professionally. This year I decided to experiment with taking a 5 day short course on Regression Modelling Strategies offered at Vanderbilt University. This blog entry is a summary of sed course material, my experience and recommendations for other Data Scientists in the industry that might be interested in diving deeper into the topic of regression. InspirationThe most common way people tend to use their development budget is by attending conferences. I also know of some coworkers who have used their development budget over several years to pursue a formal University certification in Big Data / Data Science. Personally, I have mix feelings about both and I don’t find either one to fit my learning style very well. I’ve been to couple conferences related to data science and marketing but I felt I wasn’t able to take a whole lot away. Some felt way too product and sales focused while others lacked the content that I was looking for. The challenge that I have with some of these University Big Data certificate programs is the format of the education. Typically the format is a set of courses over several semesters and classes are mandatory on a weekly basis, with midterms, assignments, and exams. What I struggle with is the pace of the learning, I feel that it is too slow for me. I also dislike the breadth and lack of depth of the content. I feel like a lot of topics are usually covered in such programs but none are explored deeply. At this point of my career, I feel I have a better sense of what I know, what I don’t know, and what I want/need to know, which has consequently made me explore alternative options for my development budget. Browsing on Twitter ultimately led me to Frank Harrell’s RMS course. I was very intrigued by the concept of a condensed course that focused on a topic that I wanted to learn more about and thought was very relevant for my day to day work. Course SummaryThe course I took is called Regression Modelling Strategies, taught by Professor Frank Harrell at Vanderbilt University. This is a condensed version of the course that he regularly teaches during the semester. The course is broken up into 1 day (optional) R tutorial and 4 full days of course content. The course is intended for Masters and PhD level students, so there is an expectation that students already have a competent understanding of multiple regression modelling. The course becomes very challenging to follow along if students do not have that understanding or are not prepared for the lectures. For more information about the course, see here. The philosophy of the course and Professor Frank Harrell can be summarized as:  It is more productive to make model fit step by step than to postulate a simple model and find out what went wrong Carefully fitting improper model is better than badly fitting a well chosen one A good overall strategy is to decide how many degrees of freedom (regression parameters) can be “spent”, where they should be spent, and to spend them with no regretsTopics covered in the course include but not limited to the following:  Hypothesis Testing vs Estimation vs. Prediction vs. Classification General Aspects of Fitting Regression Models     Interpreting Model Parameters &amp; Understanding Interactions   Relaxing Linearity Assumptions for Continuous Predictors         Non Linear Terms     Splines for Estimating Shape of Regression Function     Cubic / Restricted Cubic Splines / Choosing Number and Position of Knots     Nonparametric Regression     Assessment of Model Fit           Handling Missing Data     Strategies for Developing Imputation Model   Predictive Mean Matching    Multivariable Modelling Strategies     Variable Selection, Sample Size, Overfitting, Limit on Number of Predictors   Shrinkage   Data Reduction Techniques    Describing, Resampling, Validating and Simplifying the Model     Model Validation   Bootstrap vs. Cross Validation    Binary Logistic Regression, Ordinal Logistic Regression, Survival Analysis Case StudiesFor a more detailed list of topics, click here. Who Should Attend This Course?The course description illustrates the target audience as “statisticians and related quantitative researchers who want to learn some general model development strategies, including approaches to missing data imputation, data reduction, model validation, and relaxing linearity assumptions”. In my opinion, I feel this topic / course is suitable for anyone who is working with data, primarily focusing on problems that require a good understanding of how variables interact with each other and affect outcome. My domain of interest is marketing and I am interested in answering questions such as, what levers affect churn or LTV, and at what magnitude? How does each marketing channel influence customer conversion? This is a traditional statistics course, with a focus on biostatistics, and not a machine learning / data science course. I don’t recommend this for anyone that is only interested in the prediction aspect of modelling and not the interpretation aspect. Although this is a biostatistics course, I think the content is relevant for people outside of the medical field as well. My class comprised of primarily doctors, postgraduate students, researchers or people from the medical industry, but there were also people from the finance sector, geology, etc. In my opinion, I actually think the biostatistics aspect of the course is beneficial and makes the course much more interesting. Surrounding yourself with people that are not in your field and looking at problems that you don’t look at on a day to day basis helps stimulate thoughts and creativity. RecommendationsOverall I highly recommend other people in the industry to take this course if they are interested in learning more about regression modelling and also have similar reservations towards conferences and long form education. I think there are couple things that can really help enhance this experience (from the perspective of someone coming from industry and not academia):  Going as a team (4-5 people): Not only is this option more cost efficient, I think you can learn a lot more when you are studying as a group, by asking each other questions, sharing ideas and thoughts, challenging ideas etc.  Being very prepared for the course so that you can have meaningful conversations with Professor Frank Harrell. There were several students that were taking the course for the 2nd or 3rd time and I noticed that they were able to have a deep conversations with the Professor about specific problems they are working on. There is a textbook that comes with this course, and I would highly encourage going over that material before attending the course as well during the course.  Course improvements mentioned below. ImprovementsThe course is not perfect and there are definitely improvements that can be made. I’ve provided the same feedback on the course feedback survey.  The R tutorial is not necessary. I would prefer the R tutorial to be removed and allot an extra day for the course. The R content can be sent out prior to the course for students to review individually There is a lack of interaction / collaboration amongst the students. I think having students from very diverse fields in one place is in itself a very valuable. The concept of having a mini group project intrigues me. Replacing the R tutorial day with extra half day of course content and half day of project presentation is also an intriguing idea Go deeper into survival analysis and ordinal regression Price for non institute members is fairly expensive and does not include airfare and accomodations, so that is something to considerTakeawayI am very happy that I experimented with using my development budget on a short course. One thing to keep in mind is that you should not make the assumption that after a 5 day course you will know everything about regression modelling. That is simply impossible.  You will need to continue to self study but the course provides you with the foundation to continue exploring this topic further. I am currently still reading/re-reading the course textbook and course notes. At the same time I am reading academic papers that apply regression in the realm of marketing and I find that I am able to understand the methodologies behind the modelling process. If you have any thoughts/comments/questions about my experience, feel free to reach out over Twitter or email. I would love to help address any questions or further share with you my experience. If you know of any other good short courses, please please please reach out. I would love to learn more about it! "
    }, {
    "id": 10,
    "url": "/DS-kaggle_titanic/",
    "title": "Remember Jack and Rose?",
    "body": "2018/01/28 - Remember Jack &amp; Rose? Leo &amp; Kate Winslet? Titanic movie? Nostalgia? I wish this project was half as romantic as the movie but I’d be lying to you if I said it was. TL;DR: This exercise is the infamous Titanic dataset that all Kaggle newbs start out with. It is a classification problem where we need to predict if passengers will survive. Accuracy is used to evaluate predictions. At the time of submission, I was ranked 391/9642, top 4% on the leaderboard with an accuracy of 81. 8%. By no means is this a perfect submission, nor did I write beautiful Python code. At the end, I have a section on considerations and improvements.  I won’t include all the analysis and code in this post, just the key areas of my analysis and prediction. If you want the detailed step by step, checkout my GitHub here. Table of Contents:  Introduction Feature Exploration / Cleaning / Engineering     Sex   Embarked   Name   SibSp &amp; Parch   Cabin   Ticket   Age   Fare    Modelling &amp; Making Predictions Results Considerations / ImprovementsIntroduction: As mentioned, the goal here is the predict whether passengers will survive this accident. For more information about this kaggle competition, you can checkout https://www. kaggle. com/c/titanic. A list of passenger information is broke into two datasets, training and test. The training set has information on whether a passenger survived while in the test set this information is hidden. We will use the training set to train our prediction model and apply onto the test set and kaggle will provide an accuracy score. Information provided in the data sets:  PassengerID: ID of the passenger Survived: If a passenger survived Pclass: Ticket class Name: Passenger’s name Sex: Gender Age: Age of passenger SibSp: Number of siblings and/or spouses Parch: Number of parents and/or children Ticket: Ticket number Fare: Dollar amount of ticket Cabin: Cabin unit and number Embarked: Boarding port         PassengerId   Survived   Pclass   Name   Sex   Age   SibSp   Parch   Ticket   Fare   Cabin   Embarked         0   1   0   3   Braund, Mr. Owen Harris   male   22. 0   1   0   A/5 21171   7. 2500   NaN   S       1   2   1   1   Cumings, Mrs. John Bradley (Florence Briggs Th. . .    female   38. 0   1   0   PC 17599   71. 2833   C85   C       2   3   1   3   Heikkinen, Miss. Laina   female   26. 0   0   0   STON/O2. 3101282   7. 9250   NaN   S   Total records in train &amp; test: 1309Train: Total Survived 342Train: Total People 891 Feature Exploration / Cleaning / Engineering : Here we’ll go through one feature at a time, exploring it, cleaning it if necessary and do feature engineering if it make sense. Sex: First off, we see from above Sex is a categorical variable so I’ll define a simple function map it to numerical values. It would be interesting to just see if there is a correlation between gender and survival. We see females has a significantly higher chance of survival. def map_sex(df, col):  return df[col]. map(lambda x: 1 if x == 'female' else 0)train. groupby('Sex')['Survived']. aggregate((np. sum, len, np. mean))         sum   len   mean       Sex                  0   109   577   0. 188908       1   233   314   0. 742038   Embarked: We noticed that Embarked has one missing value in the training set. Since its a categorical, it would intuitively make sense that we can use the probability of the missing value being S, C, Q. We noticed that there is the highest chance of the missing value being S. Segmenting this by Survived, we see people Embarked = C is slightly correlated with survival rate vs S &amp; Q sns. countplot(x= Embarked , data=train, palette= Greens_d ); sns. barplot(x= Embarked , y= Survived , hue= Embarked , data=train, ci=None); Name: Name is a little more interesting. At face value, there are many different names but we see they all have a title. We know Mr. is different than Mrs. and we also see some interesting titles like Master. and Lady. Doing some research on Google we can see how these titles are used. We also notice that there are a lot of titles with similar meaning as well as titles that identify personnel with a different social class. We can group these titles together into intuitive categories to see if they have correlation with survival rate. def extract_title(df, col):  return df[col]. str. extractall('([A-Z]+[a-z]+\. )'). unstack()[0]sns. countplot(y= Title , data=train, palette= Greens_d ); Mr.     240Miss.    78Mrs.     72Master.   21Col.     2Rev.     2Dona.     1Dr.      1Ms.      1Name: Title, dtype: int64One thing to notice here is that Dona. is in test data but not the training set. We should include that in our categorization. Based on intuitive grouping, I’ve essentially classified all titles that are used for people with certain social class as Noble and anything else as Other. Similar to gender, we see female titles Mrs. and Miss have strong correlation with survival. People with noble title tend to have a higher chance of surviving, which intuitively make sense. The so called “important” people tend to get favoured treatments. title_map = {  'Mme. ': 'Mr. ',  'Mlle. ': 'Miss. ',  'Ms. ': 'Miss. ',  'Sir. ' : 'Noble',  'Rev. ' : 'Other',  'Major. ': 'Other',  'Lady. ': 'Noble',  'Jonkheer. ': 'Noble',  'Dr. ': 'Other',  'Countess. ' : 'Noble',  'Col. ': 'Other',  'Capt. ': 'Other',  'Don. ': 'Noble',   'Dona. ': 'Noble',  'Master. ': 'Noble',  'Mr. ': 'Mr. ',  'Mrs. ': 'Mrs. ',  'Miss. ': 'Miss. '}def map_title(df, col):  return df[col]. map(lambda x: title_map[x] if x in title_map. keys() else 'Other')sns. barplot(x= Title , y= Survived , hue= Title , data=train, ci=None); SibSp &amp; Parch: These two variables give us information about the passengers family, (spouse, siblings, parents, children). The tricky part is we don’t really know if someone has a sibling or a spouse, similarly a parent or a child. We can probably take some time to infer, which may give us better insight into whether that person is a mother with certain number of child. This could be a good signal since we know mothers and children tend to be saved first in situations like Titanic. Given that blurb, in this exercise, I’ve kept it basic. sns. countplot(y= SibSp , data=train, palette= Greens_d ); train. groupby('SibSp')['Survived']. aggregate((np. sum, len, np. mean))         sum   len   mean       SibSp                  0   210   608   0. 345395       1   112   209   0. 535885       2   13   28   0. 464286       3   4   16   0. 250000       4   3   18   0. 166667       5   0   5   0. 000000       8   0   7   0. 000000   sns. countplot(y= Parch , data=train, palette= Greens_d ); train. groupby('Parch')['Survived']. aggregate((np. sum, len, np. mean))         sum   len   mean       Parch                  0   233   678   0. 343658       1   65   118   0. 550847       2   40   80   0. 500000       3   3   5   0. 600000       4   0   4   0. 000000       5   1   5   0. 200000       6   0   1   0. 000000   We see pasengers with one sibling or spouse have a slight correlation with survival. Similarly for passengers with 1 parent or child. One thing we can also look at is what both of these variables together tell us. Together will identify the family size of the passenger. We see more interesting results if we look at family. People with family size of 3 is highly correlated with surviving, while people with family size of 1 and 2 have a slight correlation with survival. train['Family Size'] = train['SibSp'] + train['Parch']test['Family Size'] = train['SibSp'] + train['Parch']train. groupby('Family Size')['Survived']. aggregate((np. sum, len, np. mean))         sum   len   mean       Family Size                  0   163   537   0. 303538       1   89   161   0. 552795       2   59   102   0. 578431       3   21   29   0. 724138       4   3   15   0. 200000       5   3   22   0. 136364       6   4   12   0. 333333       7   0   6   0. 000000       10   0   7   0. 000000   Cabin: As seen above, we noticed that there are a lot of missing data points for cabin. Although I just replace this information with unknown, I can probably do a better job of inferring what cabin person could be in. For example inferring which passengers belong to the same family and use knowledge that family prefer to stay close to each other. Maybe we can use name and see which members have the same last name, look at SibSP, Parch and Family size, are they in the same Pclass. For cabin, I’m only interested in the first letter of the cabin. Reason being, the letter usually denotes the section of the cabin which implies different areas of the boat. Intuitively, even in the Titanic movie, people in different areas of the boat had different survival rate. def parse_cabin (df, col):  return df[col]. map(lambda x: re. sub(r'[0-9]+', '', x)[0])train['Cabin - Parsed'] = parse_cabin(train, 'Cabin')test['Cabin - Parsed'] = parse_cabin(test, 'Cabin')train. groupby('Cabin - Parsed')['Survived']. aggregate((np. sum, len, np. mean))         sum   len   mean       Cabin - Parsed                  A   7   15   0. 466667       B   35   47   0. 744681       C   35   59   0. 593220       D   25   33   0. 757576       E   24   32   0. 750000       F   8   13   0. 615385       G   2   4   0. 500000       T   0   1   0. 000000       U   206   687   0. 299854   test['Cabin - Parsed']. value_counts()U  327C   35B   18D   13E   9F   8A   7G   1Name: Cabin - Parsed, dtype: int64One thing to notice here is that the test set does not have cabin T. As a result, any cabin room letters that are not in the test set I will set as U (for Unknown). The only case here is T train['Cabin - Parsed'] = train['Cabin - Parsed']. map(lambda x: x if x in test['Cabin - Parsed']. value_counts(). index. tolist() else 'U')Ticket: The approach that I’ve taken here is to look at the ticket label. Similar intuition as cabin, ticket label usually identify a certain type of ticket for certain types of passenger. Looking at the ticket values below, we see that there are a lot of clean up work to be done, like cleaning up empty spaces. I’ve made a simple assumption here that the periods are just noise. One hypothesis is that tickets were hand written back then so there could of been more variation to how tickets were labelled, like some people wrote A and some wrote A (period). Tickets with no identifier I’ve given it a no identifier label. Ticket labels with a / I’ve considered it to have two labels and introduced a label count variable. train['Ticket']. head(30). unique()array(['A/5 21171', 'PC 17599', 'STON/O2. 3101282', '113803', '373450',    '330877', '17463', '349909', '347742', '237736', 'PP 9549',    '113783', 'A/5. 2151', '347082', '350406', '248706', '382652',    '244373', '345763', '2649', '239865', '248698', '330923', '113788',    '347077', '2631', '19950', '330959', '349216'], dtype=object)def ticket_split(x):  x_split = x. split(' ')  if len(x_split) &gt; 1:    return re. sub('\. ', '', x_split[0]). lower()  else:    return 'No Identifier'def ticket_first_label(x):  return x. split('/')[0]def ticket_label_count(x):  if x == 'No Identifier':    return 0  else:    return len(x. split('/'))train['Ticket - Parsed'] = train['Ticket']. map(lambda x: ticket_split(x))train['Ticket - First Label'] = train['Ticket - Parsed']. map(lambda x: ticket_first_label(x))train['Ticket Label Count'] = train['Ticket - Parsed']. map(lambda x: ticket_label_count(x))train['Ticket - First Label']. value_counts()No Identifier  665pc        60ca        42a         26ston       18soton       17sc        16w         10c         5fcc        5soc        5so         4pp         3a5         2we         2sw         2p         2a4         1fa         1sp         1fc         1sco        1sop        1wep        1Name: Ticket - First Label, dtype: int64We see a lot of ticket labels that appears very infrequently in our dataset and can be treated like outliers/noise. I’ve grouped all the ticket labels that appear less than ~3% of training set into the Other category to produce a ticket label that has a more representative sample size. A lot of levels in a categorical variable can throw off prediction accuracy. We see some interesting correlation between ticket label PC and surviving. train['Ticket - First Label'] = train['Ticket - First Label']. map(lambda x: x if x in ['No Identifier', 'pc', 'ca', 'a'] else 'Other' )train. groupby(('Ticket - First Label'))['Survived']. aggregate((np. sum, len, np. mean))         sum   len   mean       Ticket - First Label                  No Identifier   255   665   0. 383459       Other   32   98   0. 326531       a   2   26   0. 076923       ca   14   42   0. 333333       pc   39   60   0. 650000   Age: Age is probably the most interesting attribute here. We have around 150 missing records and we need to do something with them. One simple way of handling this is to simple populate it with a sample statistic like mean or median. This would lose a lot of information (~1/8 of the training set would be labeled with median / average age). Instead we can consider segmenting our data set to see if we can find more meaning ways of inferring age. First, lets take a look if age has anything to do with survival rate. We saw that gender had a huge impact on survival, lets segment by Sex to see if female vs. male age distribution. h = sns. FacetGrid(train, row='Sex', col= Survived , margin_titles=True)h. map(plt. hist,  Age , color= steelblue , bins=85, lw=0)&lt;seaborn. axisgrid. FacetGrid at 0x1156b4410&gt; Results are fairly intuitive, young to middle age male has the lowest chance of survival, while young to middle age women has the highest chance. This is one variable for our segmentation. Other variables I am considering are Title and Family size. Title make sense because a title like Miss vs Mrs sometimes can give you some indication of the age range of an individual. Family size also make sense because certain age groups tend to travel certain way. E. g people travelling individually tend to fall into a younger a group while people travelling with a family tend to fall into higher age group. Because family size is a continuous variable, It would be easier to segment our data by labelling and converting it into a categorical variable. I’ve defined three labels:  Individuals (Family size = 0) Small Family (Family size between 1 and 3) Big Family (Family size above 3)Like we expected, we see very different results based on our segmentation, which we would of missed if we simply replaced the missing values with a blanket sample statistic. A teenage girl travelling with family we would call her Miss and simultaneously a young professional female that’s not married travelling individually we would also call her Miss. def define_indiv (df, col):  return df[col]. map(lambda x: 1 if x == 0 else 0)def small_family (df, col):  return df[col]. map(lambda x: 1 if x &gt; 0 and x &lt;=3 else 0)def big_family (df, col):  return df[col]. map(lambda x: 1 if x &gt;=4 else 0)segment_age = train. groupby(['Sex', 'Title', 'Individual', 'Big Family', 'Small Family'])['Age']. aggregate((np. mean, np. median, len))segment_age                     mean   median   len       Sex   Title   Individual   Big Family   Small Family                  0   Mr.    0   0   1   32. 681818   31. 0   109. 0       1   0   27. 750000   17. 5   11. 0       1   0   0   32. 388316   29. 0   397. 0       Noble   0   0   1   6. 174762   3. 0   23. 0       1   0   5. 250000   4. 0   18. 0       1   0   0   39. 000000   39. 0   2. 0       Other   0   0   1   49. 200000   50. 0   5. 0       1   0   0   45. 363636   51. 0   12. 0       1   Miss.    0   0   1   15. 901961   15. 0   59. 0       1   0   12. 000000   9. 0   23. 0       1   0   0   27. 654321   26. 0   103. 0       Mr.    1   0   0   24. 000000   24. 0   1. 0       Mrs.    0   0   1   34. 243902   33. 0   95. 0       1   0   40. 000000   40. 0   10. 0       1   0   0   41. 812500   41. 0   20. 0       Noble   0   0   1   48. 000000   48. 0   1. 0       1   0   0   33. 000000   33. 0   1. 0       Other   1   0   0   49. 000000   49. 0   1. 0   Based on the segmentation, I take the median age in each group and build a lookup dictionary. Then I simply go through missing data in the training data and test data and fill in with the inferred age. age_median_map = segment_age['median']. to_dict()age_median_map{(0, 'Mr. ', 0, 0, 1): 31. 0, (0, 'Mr. ', 0, 1, 0): 17. 5, (0, 'Mr. ', 1, 0, 0): 29. 0, (0, 'Noble', 0, 0, 1): 3. 0, (0, 'Noble', 0, 1, 0): 4. 0, (0, 'Noble', 1, 0, 0): 39. 0, (0, 'Other', 0, 0, 1): 50. 0, (0, 'Other', 1, 0, 0): 51. 0, (1, 'Miss. ', 0, 0, 1): 15. 0, (1, 'Miss. ', 0, 1, 0): 9. 0, (1, 'Miss. ', 1, 0, 0): 26. 0, (1, 'Mr. ', 1, 0, 0): 24. 0, (1, 'Mrs. ', 0, 0, 1): 33. 0, (1, 'Mrs. ', 0, 1, 0): 40. 0, (1, 'Mrs. ', 1, 0, 0): 41. 0, (1, 'Noble', 0, 0, 1): 48. 0, (1, 'Noble', 1, 0, 0): 33. 0, (1, 'Other', 1, 0, 0): 49. 0}def compile_age_segment_key (df, col1, col2, col3, col4, col5):  return list(zip(df[col1], df[col2], df[col3]))def age_map(df, age_col, lookup_col):  if df[age_col] == -1:    if df[lookup_col] in age_median_map. keys():      return age_median_map[df[lookup_col]]    else:      return 28  else:    return df[age_col]Fare: Fare only has one missing value in the training set so we can simply fill it with a sample statistic. I chose median here because average fare price can be thrown off by couple VIP super expensive tickets. print train['Fare']. mean()print train['Fare']. median()32. 204207968614. 4542Modelling and Making Prediction: Alright now that we have cleaned up our features. We have a lot of extra columns when we were doing feature engineering and we won’t be using everything. Let’s only look at what we want to use. feature = ['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', \           'Fare', 'Embarked', 'Title', 'Family Size', 'Individual', \           'Cabin - Parsed', 'Ticket - First Label', 'Ticket Label Count',\           'Small Family', 'Big Family']target = ['Survived']train_cleaned = train[feature + target]Because sklearn models doesn’t like categorical variables, we need to turn the categorical variables into individual binary vectors. encode = ['Embarked', 'Title', 'Cabin - Parsed', 'Ticket - First Label', 'Pclass']train_cleaned = pd. get_dummies(train_cleaned, columns=encode)test_cleaned = pd. get_dummies(test_cleaned, columns=encode)all_name = train_cleaned. columnsfeature_name = all_name. drop(['Survived'])target_name = 'Survived'X_train = train_cleaned[feature_name]y_train = train_cleaned[target_name]I’ve decided to go with tree based based classified for several, but not perfect, reasons. Firstly because tree classifiers provide out-of-the-box tool to identify feature importance based on information gain in the algorithm. This is handy as we can fine tune the model by eliminating not important features that could overfit our model. Secondly I specifically chose RandomForest for its robustness. This is not to say my reasonings are perfect, I should definitely take the time to explore additional models and benchmark it against each other to compare overall predictability. from sklearn. ensemble import RandomForestClassifierfrom sklearn. feature_selection import SelectFromModelclf = RandomForestClassifier(n_estimators=200)clf = clf. fit(X_train, y_train)feature_importance = pd. DataFrame()feature_importance['Feature'] = X_train. columnsfeature_importance['Importance'] = clf. feature_importances_feature_importance. sort('Importance', ascending=False)         Feature   Importance         0   PassengerId   0. 144271       5   Fare   0. 138941       2   Age   0. 127578       15   Title_Mr.    0. 109264       1   Sex   0. 100539       14   Title_Miss.    0. 039642       34   Pclass_3   0. 035005       16   Title_Mrs.    0. 033395       6   Family Size   0. 032260       26   Cabin - Parsed_U   0. 026881       3   SibSp   0. 019907       32   Pclass_1   0. 017717       9   Small Family   0. 015980       4   Parch   0. 014885       10   Big Family   0. 013341       13   Embarked_S   0. 012709       8   Ticket Label Count   0. 012216       33   Pclass_2   0. 011202       11   Embarked_C   0. 010485       17   Title_Noble   0. 009934       7   Individual   0. 008314       27   Ticket - First Label_No Identifier   0. 007968       12   Embarked_Q   0. 007566       23   Cabin - Parsed_E   0. 006788       28   Ticket - First Label_Other   0. 006511       20   Cabin - Parsed_B   0. 005857       22   Cabin - Parsed_D   0. 005812       18   Title_Other   0. 005701       21   Cabin - Parsed_C   0. 005364       31   Ticket - First Label_pc   0. 004154       29   Ticket - First Label_a   0. 003145       30   Ticket - First Label_ca   0. 002252       19   Cabin - Parsed_A   0. 002128       24   Cabin - Parsed_F   0. 001254       25   Cabin - Parsed_G   0. 001032   The first classifier is to understand the feature importance based on information gain. Using this information, I remove variables that has less than median importance between all features. Using the remaining features, I retrain the Random Forest Classifier. Using the GridSearch approach provided by sklearn, I do a 10 fold stratified cross validation with different model parameters to see which one gives back the highest accuracy score. This requires a lot of computational resource so I’ve kept the number of parameters to tune to a limited number. After pruning, we’re left with 18 features (~50%) for our Random Forest model model = SelectFromModel(clf, prefit=True, threshold='median')X_train_pruned = model. transform(X_train)X_train_pruned. shape(891, 18)Results: from sklearn. model_selection import cross_val_scorefrom math import sqrtfrom sklearn. model_selection import StratifiedKFoldfrom sklearn. model_selection import GridSearchCVforest = RandomForestClassifier(max_features='auto')parameters = {  'n_estimators': [200, 400],  'max_depth': [7,8,9],  'max_features': ['auto', 'sqrt', 0. 2, 0. 1, 0. 3]}grid_search = GridSearchCV(forest, param_grid = parameters, cv=StratifiedKFold(n_splits=10), scoring='accuracy')grid_search. fit(X_train_pruned, y_train)From the grid search and 10 fold cross validation, we see that setting a max depth of 9, considering 20% of all features when looking for best split, and 200 estimators in our Random Forest produces the best average accuracy score of 83. 6%. Note that this score is not reflective of the actual score that you will get back from kaggle as cross validation is done on the training set. grid_search. best_score_0. 83613916947250277grid_search. best_params_{'max_depth': 9, 'max_features': 0. 2, 'n_estimators': 200}test['Survived'] = pd. Series(grid_search. predict(X_test))test[['PassengerId', 'Survived']]. to_csv('titanic_prediction. csv')Considerations / Improvements: As mentioned in this post, the solution here is not perfect and there are many improvements and iterations that can be done on top of this to get a better score. There are a lot of people in this Kaggle competition that has a perfect score, which shows that there are ways to go with this solution. To summarize, if I had more time in the future, I would love to explore different improvements including:  Use different models including logistic regression, SVM, gradient boosting etc to see if I can get better results Finding new and more useful features. E. g. is the passenger a parent? does last name (representing family and social class) have an effect on survival? Use machine learning for filling in missing data like age. E. g. using KNN to find top X similar ppl to infer ageIf you have any suggestions, ideas, questions, comments, feel free to shoot me a note and we can chat more! "
    }, {
    "id": 11,
    "url": "/DS-marvel_cinematic_plotly/",
    "title": "Marvel Cinematic Data Visualization With Plot.ly",
    "body": "2017/06/01 - I’m a huge sucker for Marvel cinematic and in this article I will do a fun exercise with building a simple interactive 3D network graph based on the relationship between Marvel characters. I will be using one of my favourite plotting libraries in Python, Plot. ly. Plot. ly is very easy to use and the way graphs are constructed is very intuitive. The dataset can be found on my GitHub or at the following link: https://www. kaggle. com/csanhueza/the-marvel-universe-social-network Setting up and exploring the data: import plotly. plotly as pyfrom plotly. graph_objs import *import plotly. offline as offlineimport pandas as pdKey thing to remember with Plot. ly is if you want to build graphs locally on you computer using Jupyter notebooks, you need to initiate offline notebook mode offline. init_notebook_mode()The hero-network. csv dataset contains two columns, hero1 and hero2 and represents a connection between the two characters. heros = pd. read_csv('hero-network. csv')heros. describe()         hero1   hero2         count   574467   574467       unique   6211   6173       top   CAPTAIN AMERICA   CAPTAIN AMERICA       freq   8149   8350   Some quick data cleaning to remove empty spaces for i in range(0,2):  heros[heros. columns[i]] = heros[heros. columns[i]]. map(lambda x: x. rstrip())If you are playing around with the same code and want to explore additional characters, you can use the following line to explore what type of characters are included in the dataset #heros[heros['hero1']. str. contains('DAREDEVIL')]['hero1']. unique()For this exercise, I’m interested in looking at the connection between some of the marvel characters and villains that we’ve see in theatres! avengers_name = ['HULK/DR. ROBERT BRUC', 'BLACK WIDOW/NATASHA', 'CAPTAIN AMERICA', 'IRON MAN/TONY STARK',         'WAR MACHINE II/PARNE', 'HAWKEYE | MUTANT X-V', 'FALCON/SAM WILSON',         'SCARLET WITCH/WANDA', 'VISION', 'ANT-MAN II/SCOTT HAR', 'SPIDER-MAN/PETER PAR',          BLACK PANTHER/T'CHAL , 'DR. STRANGE/STEPHEN', 'THOR IV/DARGO', 'FURY, COL. NICHOLAS',         'QUICKSILVER/PIETRO M'        ]villain_name = [ 'BUCKY/BUCKY BARNES', 'MALEKITH/MALCOLM KEI', 'THANOS', 'ULTRON',         'LOKI [ASGARDIAN]', 'BARON MORDO/KARL MOR', 'DORMAMMU', 'RED SKULL/JOHANN SCH']all = []all. extend(avengers_name)all. extend(villain_name)First lets just explore The Avenger’s social circle: We do a quick count on how many relationships each avengers have and how many comic books they appeared in avenger_info = {'hero': [], 'buddies': []}for i in avengers_name:  avenger_info['hero']. append(i)  avenger_info['buddies']. append(len(heros[heros['hero1']==i]))avenger_df = pd. DataFrame(avenger_info, columns = ['hero', 'buddies', 'comics'])Building a grouped bar chart in Plotly is very simple. Each set of bar is treated as seperate data, we define the x and y values and the aesthetics for each group of bars. Then we define the layout like axes, chart title etc. Lastly we pass data and layout into Plot. ly’s figure function to build the graph buddies = Bar(  x = avenger_df['hero'],  y = avenger_df['buddies'],  name = 'buddies',  marker=dict(    color='rgba(234, 35, 40, 0. 7)',    line=dict(      color='rgba(234, 35, 40, 1. 0)',      width=1    )  ))data = [buddies]layout = Layout(  barmode = 'group',  bargroupgap=0. 1,  title = 'Avengers - Buddies')fig = Figure(data = data, layout = layout)offline. iplot(fig)We can see our fellow Captain Steve Rogers is quite popular along with friendly neighborhood Spiderman and Mr. Tony Stark. One weird data point here is Thor who in my mind is should be quite popular… This might be because there are several Thor characters in the dataset, representing different Thors from different universes, as well as the comic book universe being different from cinematics. Now lets build the network graph for our Avengers and villains: First off, there are a whole lot of characters in our dataset, lets just keep our avengers and villains def keep_avengers(x, characters):  if ((x['hero1'] in characters) and (x['hero2'] in characters)):    return 1  else:    return 0heros['keep'] = heros. apply(lambda x: keep_avengers(x, all), axis=1)heros = heros[heros['keep']==1]. drop('keep', axis=1). reset_index(). drop('index', axis=1)Next we use the igraph libary which is a library for high-performance graph generation and analysis. For more information on installation, visit http://igraph. org/python/ import igraph as igOne of the requirement to build this network graph is to express the source and destination nodes as integer values. So we start off with encoding our heros into numbers heros. head(3)         hero1   hero2         0   SPIDER-MAN/PETER PAR   HULK/DR. ROBERT BRUC       1   QUICKSILVER/PIETRO M   SCARLET WITCH/WANDA       2   QUICKSILVER/PIETRO M   IRON MAN/TONY STARK   Edges = []mapper = {}character_group = []unique_heros = pd. concat([heros['hero1'], heros['hero2']]). unique()num_unique_heros = len(unique_heros)for i in range(num_unique_heros):  mapper[unique_heros[i]] = iheros['hero1_node'] = heros['hero1']. map(lambda x: mapper[x])heros['hero2_node'] = heros['hero2']. map(lambda x: mapper[x])heros. head(3)         hero1   hero2   hero1_node   hero2_node         0   SPIDER-MAN/PETER PAR   HULK/DR. ROBERT BRUC   0   3       1   QUICKSILVER/PIETRO M   SCARLET WITCH/WANDA   1   4       2   QUICKSILVER/PIETRO M   IRON MAN/TONY STARK   1   2   Next I’d like to seperate our good guys from the bad guys visually, so let’s group them up. for j in unique_heros:  character_group. append(1) if (j in avengers_name) else character_group. append(0)Now were ready to build our nodes and links. We start off with links by putting our (source, destination) for every node into Edges and pass Edges into our igraph. The layout function here defines the overall structure of our network graph and we use ‘sphere’. There are other options like    gfr, grid_fr, grid_fruchterman_reingold: grid-based Fruchterman-Reingold layout     kk, kamada_kawai: Kamada-Kawai layout     kk_3d, kk3d, kamada_kawai_3d: 3D Kamada-Kawai layout  Play around with some of these to see the different structures. for i in range(len(heros)):  Edges. append((heros['hero1_node'][i], heros['hero2_node'][i]))G = ig. Graph(Edges, directed=False)layt=G. layout('sphere', dim=3)This part looks a little intimidating and complicated but its not so bad. Layout function helps us define the layout of the network graph, what we are doing here is populating our X, Y, Z coordinates for each node and edge to be placed into our 3D space Xn=[layt[k][0] for k in range(num_unique_heros)]# x-coordinates of nodesYn=[layt[k][1] for k in range(num_unique_heros)]# y-coordinatesZn=[layt[k][2] for k in range(num_unique_heros)]# z-coordinatesXe=[]Ye=[]Ze=[]for e in Edges:  Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends  Ye+=[layt[e[0]][1],layt[e[1]][1], None]  Ze+=[layt[e[0]][2],layt[e[1]][2], None]Same as how we built the bar chart, we define our data &amp; layout and pass into the Figure function. Instead of bar chart, we are using Scatter3d for our data trace1=Scatter3d(x=Xe,        y=Ye,        z=Ze,        mode='lines',        line=Line(color='rgb(125,125,125)', width=1),        hoverinfo='none'           )trace2=Scatter3d(x=Xn,        y=Yn,        z=Zn,        mode='markers',        name='actors',        marker=Marker(symbol='dot',               size=13,               color=character_group,               colorscale=[                [0, 'black'],                [1, 'red']],               line=Line(color='rgb(50,50,50)', width=0. 5),               opacity = 0. 8               ),        text=unique_heros,        hoverinfo='text'       )axis=dict(showbackground=False,     showline=False,     zeroline=False,     showgrid=False,     showticklabels=False,     title='',     showspikes = False     )layout = Layout(     title= Marvel Cinematic - Social Network ,     width=1000,     height=700,     showlegend=False,     scene=Scene(     xaxis=XAxis(axis),     yaxis=YAxis(axis),     zaxis=ZAxis(axis)    ),    hovermode='closest'  )data=Data([trace1, trace2])fig=Figure(data=data, layout=layout)offline. iplot(fig)"
    }, {
    "id": 12,
    "url": "/DS-dimension_reduction_pca/",
    "title": "Dimension Reduction with Principal Component Analysis",
    "body": "2017/05/01 - PCA is one of the most popular techniques for dimensionality reduction. If you have no idea what I mean by dimensionality reduction, check out part 1 of this topic. In this article, we’ll explore PCA with a more applied approach rather than mathematical and we’ll keep certain details in a blackbox for future discussions. What is PCA? PCA is an algorithm that transforms a dataset to lower dimensional dataset. The keyword here is transform which is different from feature selection that I’ve talked about in part 1. The difference being that features are not removed from the dataset but instead the dataset itself is transformed. Simple hypothetical example: We have on the left the original dataset comprising of length, width, and height in centimetres. If we were to apply PCA to reduce this down to two dimensional dataset, we would get something like the right where we have two features, let say, Z1 and Z2. Z1 and Z2 is not produced from removing one of the features but instead all three features (L, W, H) are transformed and Z1 and Z2 does not represent the same measurements (cm) anymore. The goal of this algorithm is to reduce the dimensions of the dataset while retaining majority of the information, meaning maximizing the amount of variance kept. Another way of interpreting this is that the algorithm looks for vector(s) when projecting the data that minimizes the overall projection error. Lets take a look at what this means: On the left we couple data points in 2D space that we hope to use PCA to reduce down to a one dimension vector. On the top right, we project the data onto the solid line. The dotted lines from the data point to the solid line is what we call the projection error. On the bottom right is another way of projecting the data onto another line. It is evident that the projection error of the bottom right graph is much bigger than the projection error of the top right. What is not as evident is that the variance of the projected data points on the line is much bigger for the top right graph vs. the bottom right. Whether it is to minimize projection error or maximize variance, PCA’s goal is to retain as much information about the dataset as possible. For this reason, one important step before applying this algorithm is to perform normalization on the mean and variance for each feature and scaling on the range. A feature (before scaling) with a disproportionately large variance will be tend to be favoured in PCA because to the algorithm this feature explains majority of the variance in the original dataset. Now that you have a high level conceptual understanding of PCA, checkout the notebook belong on practical examples of how to apply PCA with sci-kit learn! I will illustrate two examples, one to highlight visually the results of PCA on a simple 2D &amp; 3D dataset and another applying PCA on a high dimensional dataset. For the high dimensional datset, I will use the same dataset from Kaggle about food nutrients that I’ve used previously. https://www. kaggle. com/openfoodfacts/world-food-factsThis data set has 134754 rows and 161 columns. One row per food product. import pandas as pdimport numpy as npfrom sklearn import datasetsfrom sklearn. decomposition import PCAimport matplotlib. pyplot as plt%matplotlib inlinefrom sklearn import preprocessingDefined # of Principal Components: We use the iris dataset from sklearn to demonstrate visually how to apply PCA on a 2D dataset and reduce to 1D and a 3D dataset into 2D. iris = datasets. load_iris()Before applying PCA, we need to normalize our data to mean of 0 and unit variance. We can use the scale function in the preprocessing module from sklearn. X_2d = iris. data[:, :2] X_2d_scale = preprocessing. scale(X_2d)print  Mean:   + str(X_2d_scale. mean(axis=0)). strip('[]')print  Standard Deviation:   + str(X_2d_scale. std(axis=0)). strip('[]')Mean: -1. 69031455e-15 -1. 63702385e-15Standard Deviation: 1.  1. Plotting the original and scaled data set. The two plot looks very similar because the range and variance of Sepal Length and Sepal Width are naturally similar. fig = plt. figure()fig. set_size_inches(15, 5)ax1= fig. add_subplot(121)ax1. scatter(X_2d[:, 0], X_2d[:, 1])ax1. set_xlabel('Sepal Length - Original')ax1. set_ylabel('Sepal Width - Original')ax2= fig. add_subplot(122)ax2. scatter(X_2d_scale[:,0], X_2d_scale[:,1])ax2. set_xlabel('Sepal Length - Scaled')ax2. set_ylabel('Sepal Width - Scaled')fig. suptitle('Sepal Length vs. Sepal Width (Original &amp; Scaled)') We want to reduce this dataset from 2D to 1D, using the PCA module from sklearn we specify to keep 1 component. pca = PCA(n_components = 1)pca_2d = pca. fit_transform(X_2d)To demonstrate more visually the results of PCA, lets take a look at reducing 3D dataset to 2D. Like previously, we first normalize our dataset X_3d = iris. data[:, :3] X_3d_scale = preprocessing. scale(X_3d)print  Mean:   + str(X_3d_scale. mean(axis=0)). strip('[]')print  Standard Deviation:   + str(X_3d_scale. std(axis=0)). strip('[]')Mean: -1. 69031455e-15 -1. 63702385e-15 -1. 48251781e-15Standard Deviation: 1.  1.  1. from mpl_toolkits. mplot3d import Axes3Dfig = plt. figure()fig. set_size_inches(15, 10)ax1 = fig. add_subplot(121, projection='3d')ax1. scatter(X_3d_scale[:, 0], X_3d_scale[:, 1], X_3d_scale[:, 2])ax1. set_xlabel('Sepal Length - Scaled')ax1. set_ylabel('Sepal Width - Scaled')ax1. set_zlabel('Petal Length - Scaled')ax2 = fig. add_subplot(122, projection='3d')ax2. scatter(X_3d_scale[:, 0], X_3d_scale[:, 1], X_3d_scale[:, 2])ax2. view_init(elev=0)ax2. set_xlabel('Sepal Length - Scaled')ax2. set_ylabel('Sepal Width - Scaled')ax2. set_zlabel('Petal Length - Scaled')fig. suptitle('Sepal Length vs Sepal Width vs Petal Length - Scaled') Specify to keep 2 principal components. After PCA, our dataset no longer represent Sepal Width, Sepal Length and Petal Length as PCA has transformed and projected our dataset into an arbitrary space, let say Z1, Z2, Z3 pca = PCA(n_components = 2)pca_3d = pca. fit_transform(X_3d_scale)fig = plt. figure()fig. set_size_inches(15, 10)ax1= fig. add_subplot(121, projection='3d')ax1. scatter(pca_3d[:, 0], pca_3d[:, 1], 0)ax1. set_xlabel('Z1')ax1. set_ylabel('Z2')ax1. set_zlabel('Z3')ax2= fig. add_subplot(122, projection='3d')ax2. scatter(pca_3d[:, 0], pca_3d[:, 1], 0)ax2. view_init(elev=0)ax2. set_xlabel('Z1')ax2. set_ylabel('Z2')ax2. set_zlabel('Z3') Solving For Number Of Principal Components: Whats powerful about sklearn is that the same modules we used previously for PCA, under the circumstances that we don’t know the number of principal components prior to applying the reduction, can be used to solve for the number of principal components required to keep a certain % of variance within the dataset. Typically we want to retain 90%, 95% or 99% of the variance but depends on use case. Lets use the same iris data as an example first then the food nutrient dataset from Kaggle. iris_3d = X_3d_scaleTo find the number of principal components, in the same PCA function, we define n_components to be 0 &lt; n_components &lt; 1 which is the % of variance that we want to keep in the dataset and we specify svd_solver == ‘full’. Lets start with 95% then 30% and see what happens p_95 = PCA(n_components = 0. 95, svd_solver='full')p_95. fit(iris_3d)PCA(copy=True, iterated_power='auto', n_components=0. 95, random_state=None, svd_solver='full', tol=0. 0, whiten=False)print  We see that to keep 95% of the variance in the original dataset, we can reduce the dataset into 2D \n print  The number of components :  + str(p_95. n_components_)print  The % of variance explained by each component:   + str(p_95. explained_variance_ratio_)We see that to keep 95% of the variance in the original dataset, we can reduce the dataset into 2D The number of components :2The % of variance explained by each component: [ 0. 67127544 0. 30494357]p_30 = PCA(n_components = 0. 30, svd_solver='full')p_30. fit(iris_3d)PCA(copy=True, iterated_power='auto', n_components=0. 3, random_state=None, svd_solver='full', tol=0. 0, whiten=False)print  We see that to keep only 30% of the variance in the original dataset, we can reduce the dataset into 1D \n print  The number of components :  + str(p_30. n_components_)print  The % of variance explained by each component:   + str(p_30. explained_variance_ratio_)We see that to keep only 30% of the variance in the original dataset, we can reduce the dataset into 1D The number of components :1The % of variance explained by each component: [ 0. 67127544]Applying PCA on food nutrient dataset from Kaggle: If you haven’t already, check out part 1 of dimensionality reduction where I’ve applied simple feature selection methods to the same dataset. food_data = pd. read_csv('openfoodfacts. tsv', sep='\t')We will do some preprocessing work on this dataset for this example:  dataset contains categorical variables, we will only explore the numerical variables for now there are variables with no data, we will remove these variables apply normalization apply feature scaling because different nutrients have different units and range of valuesfood = food_data. copy()numeric_columns = food. dtypes[food. dtypes == 'float64']. index #Extract numerical variablesfood = food[numeric_columns]missing = food. isnull(). sum() #Count number of missing valuespct_missing = 1. 0*missing/len(food) #Calculate percentagefood = food[pct_missing[pct_missing != 1. 0]. index] #Remove variables that have no datafood. shape(134754, 89)for i in food. columns:  food[i]. fillna(value=food[i]. mean(), inplace=True) #replace NaN with mean of dimension  food[i] = preprocessing. scale(food[i]) #normalization  food[i] = preprocessing. MinMaxScaler(). fit_transform(food[i]. values. reshape(-1,1)) #scale min max to 0-1There are 89 variables that we are exploring now in this processed dataset. How many do we keep? Let’s define that we want to keep 95% of the variance p = PCA(n_components = 0. 95, svd_solver='full')p. fit(food)PCA(copy=True, iterated_power='auto', n_components=0. 95, random_state=None, svd_solver='full', tol=0. 0, whiten=False)p. n_components_11p. explained_variance_ratio_array([ 0. 46515374, 0. 19124263, 0. 09001465, 0. 0692121 , 0. 04144684,    0. 03047795, 0. 01878442, 0. 0172208 , 0. 01577135, 0. 00958264,    0. 0090774 ])sum(p. explained_variance_ratio_)0. 95798452774543918As seen, we can reduce the 89 variables down to 11 components to keep 95% of the variance. The explain_variance_ratio provides information on how much variance is kept for each component. The sum of it we can see is &gt;95% which is the minimal we’ve defined. If we want more variance to be kept then # of components will increase and vice versa! "
    }, {
    "id": 13,
    "url": "/DS-dimension_reduction_examples_part1/",
    "title": "Dimension Reduction Examples - Part 1",
    "body": "2017/04/27 - What is dimensionality reduction? The name might sound fancier than what it actually is. It is simply the process of reducing the number of dimensions in a dataset aka reducing the number of attributes/features/columns while retaining key information in the dataset.  Wait, but why?  Because more data does not necessarily mean we get better model performance. Many attributes could be noise to the key signals in a dataset (overfitting, curse of dimensionality).  Data compression to reduce storage size, which reduces computational resource and helps speed up algorithm.  Reducing data to 2D or 3D allows us to visualize the data. There are two approaches to dimensionality reduction comprising different techniques/algorithms:  Feature Selection – selecting subset of feature in the dataset without transforming the dataset as a whole Feature Extraction – transforming the dataset into a lower dimensional spaceFor part 1 of dimensionality reduction, we’ll get started with applying three simple feature selection techniques using Python. import pandas as pdimport numpy as npTo explore how to apply different dimension reduction techniques in Python, I will use a data set on food nutrient facts from Kaggle as an example. This data set has 134754 rows and 161 columns. One row per food product. https://www. kaggle. com/openfoodfacts/world-food-facts food_data = pd. read_csv('openfoodfacts. tsv', sep='\t')food_data. shape(134754, 161)food_data. columnsIndex([u'code', u'url', u'creator', u'created_t', u'created_datetime',    u'last_modified_t', u'last_modified_datetime', u'product_name',    u'generic_name', u'quantity',    . . .    u'ph_100g', u'fruits-vegetables-nuts_100g',    u'collagen-meat-protein-ratio_100g', u'cocoa_100g', u'chlorophyl_100g',    u'carbon-footprint_100g', u'nutrition-score-fr_100g',    u'nutrition-score-uk_100g', u'glycemic-index_100g',    u'water-hardness_100g'],    dtype='object', length=161)food_data. head(4)         code   url   creator   created_t   created_datetime   last_modified_t   last_modified_datetime   product_name   generic_name   quantity   . . .    ph_100g   fruits-vegetables-nuts_100g   collagen-meat-protein-ratio_100g   cocoa_100g   chlorophyl_100g   carbon-footprint_100g   nutrition-score-fr_100g   nutrition-score-uk_100g   glycemic-index_100g   water-hardness_100g         0   3087   http://world-en. openfoodfacts. org/product/0000. . .    openfoodfacts-contributors   1474103866   2016-09-17T09:17:46Z   1474103893   2016-09-17T09:18:13Z   Farine de blé noir   NaN   1kg   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       1   24600   http://world-en. openfoodfacts. org/product/0000. . .    date-limite-app   1434530704   2015-06-17T08:45:04Z   1434535914   2015-06-17T10:11:54Z   Filet de bœuf   NaN   2. 46 kg   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       2   27083   http://world-en. openfoodfacts. org/product/0000. . .    canieatthis-app   1472223782   2016-08-26T15:03:02Z   1472223782   2016-08-26T15:03:02Z   Marks % Spencer 2 Blueberry Muffins   NaN   230g   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       3   27205   http://world-en. openfoodfacts. org/product/0000. . .    tacinte   1458238630   2016-03-17T18:17:10Z   1458238638   2016-03-17T18:17:18Z   NaN   NaN   NaN   . . .    NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   4 rows × 161 columns food_data. isnull(). sum()code                  23url                   23creator                 2created_t                4created_datetime            10last_modified_t             0last_modified_datetime          0product_name             18578generic_name             84411quantity               35948packaging              59313packaging_tags            59311brands                27194brands_tags             27200categories              56286categories_tags           56307categories_en            56286origins               113257origins_tags            113294manufacturing_places        100295manufacturing_places_tags      100301labels                93449labels_tags             93363labels_en              93342emb_codes              106460emb_codes_tags           106463first_packaging_code_geo      116621cities               134732cities_tags             115136purchase_places           79454                   . . .  biotin_100g             134437pantothenic-acid_100g        134074silica_100g             134717bicarbonate_100g          134676potassium_100g           134058chloride_100g            134601calcium_100g            130445phosphorus_100g           133870iron_100g              132149magnesium_100g           133475zinc_100g              134271copper_100g             134598manganese_100g           134609fluoride_100g            134676selenium_100g            134575chromium_100g            134735molybdenum_100g           134744iodine_100g             134499caffeine_100g            134705taurine_100g            134720ph_100g               134706fruits-vegetables-nuts_100g     133091collagen-meat-protein-ratio_100g  134591cocoa_100g             133904chlorophyl_100g           134754carbon-footprint_100g        134489nutrition-score-fr_100g       67502nutrition-score-uk_100g       67502glycemic-index_100g         134754water-hardness_100g         134754dtype: int64Missing Value Ratio: Attributes with a lot of missing values are not providing a lot of information. We can either impute the values for these attributes or remove from dataset. We compute the % of missing values and determine whether to drop the attribute or not. The threshold is up to you but roughly speaking an attribute with ~40-50% missing values could be dropped. We can leverage isnull function from pandas dataframe to count how many empty fields are in each column. We can also leverage this information to calculate the % of missing values for each attribute. missing = food_data. isnull(). sum()pct_missing = 1. 0*missing/len(food_data)print pct_missing. sort_values(ascending=False)water-hardness_100g           1. 000000-nervonic-acid_100g           1. 000000no_nutriments              1. 000000ingredients_from_palm_oil        1. 000000ingredients_that_may_be_from_palm_oil  1. 000000nutrition_grade_uk            1. 000000-butyric-acid_100g            1. 000000-caproic-acid_100g            1. 000000-lignoceric-acid_100g          1. 000000-cerotic-acid_100g            1. 000000glycemic-index_100g           1. 000000-elaidic-acid_100g            1. 000000-mead-acid_100g             1. 000000-erucic-acid_100g            1. 000000-melissic-acid_100g           1. 000000chlorophyl_100g             1. 000000-myristic-acid_100g           0. 999993-caprylic-acid_100g           0. 999993-montanic-acid_100g           0. 999993-stearic-acid_100g            0. 999993-palmitic-acid_100g           0. 999993-capric-acid_100g            0. 999985-lauric-acid_100g            0. 999970-maltose_100g              0. 999970nucleotides_100g             0. 999948-arachidonic-acid_100g          0. 999941molybdenum_100g             0. 999926-maltodextrins_100g           0. 999918-oleic-acid_100g             0. 999911serum-proteins_100g           0. 999896                      . . .  proteins_100g              0. 445916energy_100g               0. 440907packaging                0. 440158packaging_tags              0. 440143image_url                0. 434577image_small_url             0. 434577main_category              0. 418021main_category_en             0. 418021categories_tags             0. 417850categories                0. 417694categories_en              0. 417694pnns_groups_1              0. 372085pnns_groups_2              0. 351440quantity                 0. 266768brands_tags               0. 201849brands                  0. 201805product_name               0. 137866countries_tags              0. 002070countries                0. 002070countries_en               0. 002070states_en                0. 000341states_tags               0. 000341states                  0. 000341url                   0. 000171code                   0. 000171created_datetime             0. 000074created_t                0. 000030creator                 0. 000015last_modified_datetime          0. 000000last_modified_t             0. 000000dtype: float64It is evident there are significant number of attributes that barely have any information. Lets remove features that have less than 25% value. new_food_data = food_data[pct_missing[pct_missing &lt; 0. 75]. index. tolist()]new_food_data. columnsIndex([u'code', u'url', u'creator', u'created_t', u'created_datetime',    u'last_modified_t', u'last_modified_datetime', u'product_name',    u'generic_name', u'quantity', u'packaging', u'packaging_tags',    u'brands', u'brands_tags', u'categories', u'categories_tags',    u'categories_en', u'manufacturing_places', u'manufacturing_places_tags',    u'labels', u'labels_tags', u'labels_en', u'purchase_places', u'stores',    u'countries', u'countries_tags', u'countries_en', u'ingredients_text',    u'serving_size', u'additives_n', u'additives', u'additives_tags',    u'additives_en', u'ingredients_from_palm_oil_n',    u'ingredients_that_may_be_from_palm_oil_n', u'nutrition_grade_fr',    u'pnns_groups_1', u'pnns_groups_2', u'states', u'states_tags',    u'states_en', u'main_category', u'main_category_en', u'image_url',    u'image_small_url', u'energy_100g', u'fat_100g', u'saturated-fat_100g',    u'carbohydrates_100g', u'sugars_100g', u'fiber_100g', u'proteins_100g',    u'salt_100g', u'sodium_100g', u'nutrition-score-fr_100g',    u'nutrition-score-uk_100g'],   dtype='object')Low Variance Filter: Attributes with very little change in its data, e. g. all values are 1s, also provides very little information. Similar to Missing Value Ratio, we remove attributes based on a define threshold of variance. Variance is range dependent therefore normalization is required and only applicable to numerical attributes. We need to normalize each dimension as variance is range dependent. We can use the MinMaxScaler function from sklearn preprocessing module to normalize value in each dimension to a value between 0 and 1. The challenge with this is that sklearn estimators does not handle NaN or missing values. An intermediate step is required to infer missing data with either mean or median or whatever statistics that would make most sense. There are different ways to do this like using fillna() function in pandas or Imputer module from sklearn. Another method we can simple define our own normalization function. from sklearn import preprocessingvar_fil_food_data = new_food_data. copy()scaler = preprocessing. MinMaxScaler()#Extract numeric columns because cannot normalize and compute variance on categoricalnumeric_columns = var_fil_food_data. dtypes[var_fil_food_data. dtypes == 'float64']. indexfor i in numeric_columns:  var_fil_food_data[i]. fillna(value=var_fil_food_data[i]. mean(), inplace=True) #replace NaN with mean of dimension  var_fil_food_data[i] = scaler. fit_transform(var_fil_food_data[i]. values. reshape(-1,1))   #Normalize. if we don't use . values. reshapes it still works but sklearn throws depracated warningvar_fil_food_data[numeric_columns]. mean()additives_n                0. 054490ingredients_from_palm_oil_n        0. 031198ingredients_that_may_be_from_palm_oil_n  0. 020675energy_100g                0. 012728fat_100g                  0. 133428saturated-fat_100g             0. 054690carbohydrates_100g             0. 072374sugars_100g                0. 130803fiber_100g                 0. 027809proteins_100g               0. 075218salt_100g                 0. 001004sodium_100g                0. 001005nutrition-score-fr_100g          0. 427915nutrition-score-uk_100g          0. 457145dtype: float64var_fil_food_data[numeric_columns]. var()additives_n                0. 003423ingredients_from_palm_oil_n        0. 008250ingredients_that_may_be_from_palm_oil_n  0. 002640energy_100g                0. 000061fat_100g                  0. 014988saturated-fat_100g             0. 003620carbohydrates_100g             0. 002530sugars_100g                0. 017339fiber_100g                 0. 000658proteins_100g               0. 003247salt_100g                 0. 000020sodium_100g                0. 000020nutrition-score-fr_100g          0. 013776nutrition-score-uk_100g          0. 017170dtype: float64Looking at the mean and variance, we could explore removing energy_100g, salt_100g sodium_100g. new_food_data = new_food_data. drop(['energy_100g', 'salt_100g', 'sodium_100g'], axis=1)Correlation Filter: Attributes that are highly correlated tends to carry similar information, e. g. a company’s overall spend and its marketing spend. Because highly correlated attributes contain similar information, we can keep just one of these attributes. To keep this example simple, we will only look at the correlation between numeric variables. For categorical variables, there is an additional encoding step (covered in another blog article) that is required, which simply splits every categorical value of one dimension into individual columns with binary values of 1 or 0. We can build a correlation matrix using the corr function in pandas. We could also use a more visual approach by using heatmap from seaborn library. corr_fil_food_data = new_food_data. copy()#Extract numeric columns because cannot normalize and compute variance on categoricalnumeric_columns = corr_fil_food_data. dtypes[corr_fil_food_data. dtypes == 'float64']. indexcorr_fil_food_data = corr_fil_food_data[numeric_columns]corr_fil_food_data. corr()         additives_n   ingredients_from_palm_oil_n   ingredients_that_may_be_from_palm_oil_n   fat_100g   saturated-fat_100g   carbohydrates_100g   sugars_100g   fiber_100g   proteins_100g   nutrition-score-fr_100g   nutrition-score-uk_100g         additives_n   1. 000000   0. 247840   0. 433042   -0. 027467   -0. 019006   0. 119767   0. 124980   -0. 107708   -0. 083062   0. 202419   0. 187053       ingredients_from_palm_oil_n   0. 247840   1. 000000   0. 179777   0. 108486   0. 142192   0. 211748   0. 168584   0. 011094   -0. 036060   0. 245711   0. 248023       ingredients_that_may_be_from_palm_oil_n   0. 433042   0. 179777   1. 000000   0. 042765   0. 044454   0. 122512   0. 052955   -0. 038976   -0. 058680   0. 121939   0. 125042       fat_100g   -0. 027467   0. 108486   0. 042765   1. 000000   0. 735497   -0. 071676   0. 023426   0. 082148   0. 146350   0. 591396   0. 655143       saturated-fat_100g   -0. 019006   0. 142192   0. 044454   0. 735497   1. 000000   -0. 012336   0. 121237   0. 020530   0. 131305   0. 623594   0. 664247       carbohydrates_100g   0. 119767   0. 211748   0. 122512   -0. 071676   -0. 012336   1. 000000   0. 637138   0. 246810   -0. 103593   0. 257640   0. 248387       sugars_100g   0. 124980   0. 168584   0. 052955   0. 023426   0. 121237   0. 637138   1. 000000   0. 034637   -0. 237634   0. 480360   0. 448149       fiber_100g   -0. 107708   0. 011094   -0. 038976   0. 082148   0. 020530   0. 246810   0. 034637   1. 000000   0. 230218   -0. 102295   -0. 092460       proteins_100g   -0. 083062   -0. 036060   -0. 058680   0. 146350   0. 131305   -0. 103593   -0. 237634   0. 230218   1. 000000   0. 094913   0. 156746       nutrition-score-fr_100g   0. 202419   0. 245711   0. 121939   0. 591396   0. 623594   0. 257640   0. 480360   -0. 102295   0. 094913   1. 000000   0. 967227       nutrition-score-uk_100g   0. 187053   0. 248023   0. 125042   0. 655143   0. 664247   0. 248387   0. 448149   -0. 092460   0. 156746   0. 967227   1. 000000   import seaborn as snsimport matplotlib. pyplot as plt%matplotlib inlinesns. set()fig, ax = plt. subplots()fig. set_size_inches(15, 15)sns. heatmap(corr_fil_food_data. corr(), annot=True, ) It is evident that:  nutrition-score-fr_100g is highly correlated with nutrition-score-uk_100g fat_100g is pretty correlated with saturated-fat_100g nutrition-score-uk_100g is pretty correlated with fat_100g and saturated-fat_100g nutrition-score-fr_100g is pretty correlated with fat_100g and saturated-fat_100g sugars_100g is pretty correlated with carbohydrate_100gLets remove one of these attributes fig, ax = plt. subplots()fig. set_size_inches(15, 15)sns. heatmap(corr_fil_food_data. drop(    ['nutrition-score-fr_100g',     'nutrition-score-uk_100g',     'fat_100g', 'sugars_100g'],    axis=1). corr(), annot=True, ) Result: Using these three simple techniques for dimension reduction, we’ve reduce this dataset from 161 variables down to 49. Do keep in mind that the goal of dimension reduction is to remove attributes that are not very informative. More data does not necessarily mean better and at the same time less data does not necessarily mean better as well. The art is to find a set of attributes within a high dimension data set that will provide sufficient information. new_food_data. shape(134754, 49)"
    }, {
    "id": 14,
    "url": "/DS-intro_to_python_and_pandas/",
    "title": "Intro To Python and Pandas",
    "body": "2017/02/18 - Python is one of the most popular and fastest growing software languages used in data analytics today and there are many reasons for that. I will cover majority of the topics in this blog in Python and occasionally R. For those who are completely new to Python and programming, I encourage you to check out Codeacademy and do their free Python course. It’s important to get a good feel for this language and understand some fundamental concepts like syntax, libraries, data structures etc. The course is very straight forward and suitable for all level. pandas is a Python library (prewritten code for us to use) and is the bread &amp; butter for data analysis. I’m assuming everyone that’s reading this used Excel or some sort of spreadsheet before. Pandas essentially stores data in a tabular format (rows and columns like a spreadsheet) with predefined functionalities to manipulate &amp; analyze data. After learning about basic Python, I encourage you to check out this tutorial from Greg Reda on how to use pandas. In this series of blog articles, I will instead focus on this Panda’s cheatsheet and additional common functionalities, walking through some examples and relating to how you would use it on a day to day task. Lets start with creating Series an DataFrames, getting some data and removing some data. import pandas as pdCreating Series and DataFrame: Lets first define a Series. A Series is simply a one dimensional array, kind of like your Excel spreadsheet but with one column. pd. Series([3, -5, 7, 4])0  31  -52  73  4dtype: int64The 0, 1, 2, 3 here is the index of the series. They are like the row #s in excel. We can also have predefined index labels for our Series. Lets label it a, b, c, d s = pd. Series([3, -5, 7, 4], index=['a', 'b', 'c', 'd'])sa  3b  -5c  7d  4dtype: int64DataFrame on the other hand is like a Series but two dimensional, so like your full Excel sheet instead of one column. There are many ways to create a DataFrame with your data, its quite flexible and up to you what is the easiest based on the data set you have. #if your data is already in dictionary formdata = {'Country': ['Belgium', 'India', 'Brazil'],    'Capital': ['Brussels', 'New Delhi', 'Brasília'],    'Population': [11190846, 1303171035, 207847528]}df = pd. DataFrame(data)df         Capital   Country   Population         0   Brussels   Belgium   11190846       1   New Delhi   India   1303171035       2   Brasília   Brazil   207847528   Or you could have some Series you already defined and want to put them together into a DataFrame. The NaN here means there is unknown value. s1 = pd. Series([3, -5, 7, 4])s2 = pd. Series([1, 3, 6])pd. DataFrame({    's1': s1,    's2': s2})         s1   s2         0   3   1. 0       1   -5   3. 0       2   7   6. 0       3   4   NaN   Retrieving Data: Remember previously your Series by default is indexed by number (0,1,2,3) and you also labeled the index. You can get information either way, using index number or your label print s['a']print s[0]33DataFrame is similar. Earlier we created a DataFrame with country information. We can get a column by specifying the column name df['Country']0  Belgium1   India2   BrazilName: Country, dtype: objectOr multiple columns df[['Country', 'Capital']]         Country   Capital         0   Belgium   Brussels       1   India   New Delhi       2   Brazil   Brasília   And to select a range of rows, we use the : operator. It simply means select from this index value up to this index value. E. g. 0:2 means select row index 0 and 1 df[0:2]         Capital   Country   Population         0   Brussels   Belgium   11190846       1   New Delhi   India   1303171035   Deleting Data: Sometimes we dont need all the data in our DataFrame or Series. Pandas provide a very convenient way of removing the data using the drop function s. drop('a')b  -5c  7d  4dtype: int64s. drop(['a','d'])b  -5c  7dtype: int64DataFrame has an extra parameter that you need to specif, axis. Axis = 0 means you are looking to remove a row. Axis = 1 means you want to remove a column. df. drop('Capital', axis=1)         Country   Population         0   Belgium   11190846       1   India   1303171035       2   Brazil   207847528   df. drop([1,2], axis=0)         Capital   Country   Population         0   Brussels   Belgium   11190846   A very simple function in Python is the range function, which allows you to define a range of number range([start],[stop]) range(0,2)[0, 1]df. drop(range(0,2))         Capital   Country   Population         2   Brasília   Brazil   207847528   "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script>
function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});
</script>

                
            </ul>		
        
        
  
        <!-- End Menu -->

    </div>
        
    </div>
</nav>
<!-- End Navigation
================================================== -->
    
<div class="site-content">   
    
<div class="container">
    
<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">The Art of Marketing Science</h1>
    <p class="lead">
         Data science in marketing with practical examples
    </p>
</div>

    
    
<!-- Content
================================================== --> 
<div class="main-content">
    <div class="text-center">
<h1 class="display-1 mt-5 mb-4"><span class="badge badge-danger">404</span> Page does not exist!</h1>
<p>Please use the search bar at the top or visit our <a href="/">homepage</a>!</p>
</div>

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt="The Art of Marketing Science"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://github.us20.list-manage.com/subscribe/post?u=7e19d52d70f2181f9f5f666db&amp;id=2158c5e28a" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>    

    
</div><!-- /.container> -->
    
<!-- Categories Jumbotron
================================================== -->
<!--<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">      
            
            
            
            <a href="/categories#data-sandbox">Data Sandbox (6)</a>
            
            <a href="/categories#art-of-marketing-science">Art of Marketing Science (4)</a>
            
            
            
		</div>
	</div>
</div>
-->
 


<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                 Copyright © 2019 The Art of Marketing Science 
            </div>
            <!--<div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>-->
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

   
</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
    
<script src="/assets/js/mediumish.js"></script>

<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 
    
<script id="dsq-count-scr" src="//https-fongmanfong-github-io.disqus.com/count.js"></script>
    
</body>
</html>
